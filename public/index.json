
[{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/","section":"Bishop's Corner","summary":"","title":"Bishop's Corner","type":"page"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/categories/cyber-security-mindset/","section":"Categories","summary":"","title":"Cyber Security Mindset","type":"categories"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/cyber-security/","section":"Tags","summary":"","title":"Cyber-Security","type":"tags"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/categories/learning-frameworks/","section":"Categories","summary":"","title":"Learning Frameworks","type":"categories"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/learning-principles/","section":"Tags","summary":"","title":"Learning-Principles","type":"tags"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/offensive-security/","section":"Tags","summary":"","title":"Offensive-Security","type":"tags"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/osint/","section":"Tags","summary":"","title":"Osint","type":"tags"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/categories/professional-growth/","section":"Categories","summary":"","title":"Professional Growth","type":"categories"},{"content":"\rThe Journey of Mastery: Shu Ha Ri in Cybersecurity #\rIn the quiet hours of an early morning, I sat staring at my screen, the blue light casting a glow over my increasingly frustrated expression. Another attempt at reverse-engineering a particularly elusive piece of malware had left me exhausted. Three weeks into my new role at a security firm, self-doubt began creeping in. Did I have what it takes?\nMy mentor, Eliza, noticed my struggle. “You’re trying to run before you can walk,” she said, pulling up a chair beside me. “Have you ever heard of Shu Ha Ri?”\nThe term was unfamiliar, so she explained: it’s an ancient Japanese martial arts philosophy that outlines the stages of learning on the path to mastery.\n“In Shu, you follow the rules exactly. In Ha, you break the rules with understanding. In Ri, you become the rules.”\nI nodded and listened to her guidance. For the next few months, I committed myself to disciplined imitation—the Shu phase. I meticulously followed established malware analysis procedures without deviation. Every step was documented, every tool used as intended. Slowly, I built a solid foundation.\nSix months later, something shifted. As I analyzed a network intrusion, patterns emerged that didn’t fit neatly into our playbooks. I began combining techniques in creative ways, questioning why certain approaches worked better in specific contexts. This was Ha—thoughtful adaptation.\nAfter two years of relentless practice, I reached a turning point. During an incident response for a critical infrastructure client, I found myself working almost instinctively. I wasn’t following a predefined methodology; instead, I responded fluidly to what the code revealed. Novel approaches to tracing the attack path came naturally, even surprising Eliza. This was Ri—natural innovation.\nNow, as I mentor newcomers in offensive security, I share this journey. Learning in cybersecurity isn’t linear—it’s cyclical. Even as I reach Ri in one area, like malware analysis or OSINT, I find myself back at Shu when tackling cloud security or another unfamiliar domain. The willingness to start over as a beginner, paired with the wisdom gained from past mastery, creates the mindset of continuous growth that this field demands.\nAnd so, the cycle continues: Shu, Ha, Ri—and then back to Shu once more. Because in cybersecurity, mastery isn’t a destination—it’s a continous process.\n","date":"15 April 2025","externalUrl":null,"permalink":"/posts/shu-ha-ri/","section":"Posts","summary":"Discover how the Shu Ha Ri principle—originating from martial arts—can transform your approach to learning. Understand the stages of structured learning (Shu), creative adaptation (Ha), and mastery through innovation (Ri) with practical insights for domains such as cyber security, malware analysis, and OSINT.","title":"Shu Ha Ri Principle in Learning and Mastery","type":"posts"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/shu-ha-ri/","section":"Tags","summary":"","title":"Shu-Ha-Ri","type":"tags"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/skill-development/","section":"Tags","summary":"","title":"Skill-Development","type":"tags"},{"content":"","date":"15 April 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/categories/cisco-labs/","section":"Categories","summary":"","title":"Cisco Labs","type":"categories"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/categories/network-communication/","section":"Categories","summary":"","title":"Network Communication","type":"categories"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/network-simulation/","section":"Tags","summary":"","title":"Network-Simulation","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/categories/networking-fundamentals/","section":"Categories","summary":"","title":"Networking Fundamentals","type":"categories"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/osi-model/","section":"Tags","summary":"","title":"Osi-Model","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/protocol-analysis/","section":"Tags","summary":"","title":"Protocol-Analysis","type":"tags"},{"content":"","date":"10 February 2025","externalUrl":null,"permalink":"/tags/tcp-ip/","section":"Tags","summary":"","title":"Tcp-Ip","type":"tags"},{"content":"\rVisualizing the TCP-IP and OSI Models in Network Communication #\rIntroduction to OSI model and TCP- IP model #\rThe OSI Model\nThe OSI model consists of seven layers, each with its own specific functions:\nPhysical Layer: This layer is responsible for the physical connection between devices, including the transmission of raw bitstreams over a physical medium.\nData Link Layer: This layer provides node-to-node data transfer and handles error correction from the physical layer. It ensures that data packets are transmitted without errors.\nNetwork Layer: The network layer is responsible for routing data packets between devices across different networks. It manages addressing and determines the best path for data transmission.\nTransport Layer: This layer ensures complete data transfer and error recovery. It provides end-to-end communication and can manage flow control and segmentation of data.\nSession Layer: The session layer manages sessions between applications. It establishes, maintains, and terminates connections between applications.\nPresentation Layer: This layer translates data between the application layer and the network. It is responsible for data formatting, encryption, and compression.\nApplication Layer: The topmost layer, the application layer, provides network services directly to end-user applications. It facilitates user interface and application-level protocols.\nThe OSI model was developed in 1984 by the International Organization for Standardization (ISO) and serves as a theoretical framework for understanding network interactions.\nTCP/IP Model\nThe TCP/IP model, on the other hand, consists of four layers:\nPhysical Layer: Similar to the OSI model, this layer deals with the physical aspects of data transmission over a network.\nNetwork Layer: This layer is responsible for addressing and routing packets of data across networks, ensuring that data can travel from the source to the destination.\nTransport Layer: The transport layer provides reliable data transfer services to the upper layers. It manages error detection and correction, as well as flow control.\nApplication Layer: This layer encompasses all protocols and services that applications use to communicate over the network. It includes protocols such as HTTP, FTP, and SMTP.\nThe TCP/IP model is more streamlined than the OSI model, combining some of the OSI layers into fewer layers, which reflects the practical implementation of network protocols.\nObjectives #\rUsing Cisco packet-tracer lab we will approach the project in two parts.\nPart 1: Examine HTTP Web Traffic Part 2: Display Elements of the TCP/IP Protocol Suite Background Scenario #\rTopology Image: This simulation activity is intended to provide a foundation for understanding the TCP/IP protocol suite and the relationship to the OSI model. Simulation mode allows you to view the data contents being sent across the network at each layer. As data moves through the network, it is broken down into smaller pieces and identified so that the pieces can be put back together when they arrive at the destination. Each piece is assigned a specific name (protocol data unit [PDU]) and associated with a specific layer of the TCP/IP and OSI models. Packet Tracer simulation mode enables you to view each of the layers and the associated PDU. The following steps lead the user through the process of requesting a web page from a web server by using the web browser application available on a client PC.\nEven though much of the information displayed will be discussed in more detail later, this is an opportunity to explore the functionality of Packet Tracer and be able to visualize the encapsulation process.\nPart 1: Examine HTTP Web Traffic #\rIn Part 1 of this activity, you will use Packet Tracer (PT) Simulation mode to generate web traffic and examine HTTP.\nStep 1: Switch from Realtime to Simulation mode. #\rIn the lower right corner of the Packet Tracer interface are tabs to toggle between Realtime and Simulation mode. PT always starts in Realtime mode, in which networking protocols operate with realistic timings. However, a powerful feature of Packet Tracer allows the user to “stop time” by switching to Simulation mode. In Simulation mode, packets are displayed as animated envelopes, time is event driven, and the user can step through networking events. a. Click the Simulation mode icon to switch from Realtime mode to Simulation mode.\nb. Select HTTP from the Event List Filters.\nHTTP may already be the only visible event. Click Edit Filters to display the available visible events. Toggle the Show All/None check box and notice how the check boxes switch from unchecked to checked or checked to unchecked, depending on the current state. Click the Show All/None check box until all boxes are cleared and then select HTTP. Click anywhere outside of the Edit Filters box to hide it. The Visible Events should now only display HTTP. Step 2: Generate web (HTTP) traffic. #\rCurrently the Simulation Panel is empty. There are six columns listed across the top of the Event List within the Simulation Panel. As traffic is generated and stepped through, events appear in the list. The Info column is used to inspect the contents of a particular event. Note: The Web Server and Web Client are displayed in the left pane. The panels can be adjusted in size by hovering next to the scroll bar and dragging left or right when the double-headed arrow appears.\na. Click Web Client in the far left pane. b. Click the Desktop tab and click the Web Browser icon to open it. c. In the URL field, enter www.osi.local and click Go.Because time in Simulation mode is event-driven, you must use the Capture/Forward button to display network events. d. Click Capture/Forward four times. There should be four events in the Event List.Look at the Web Client web browser page. Did anything change?The web page was returned from the web server. Step 3: Explore the contents of the HTTP packet. #\ra. Click the first colored square box under the Event List \u0026gt; Info column. It may be necessary to expand the Simulation Panel or use the scrollbar directly below the Event List.The PDU Information at Device: Web Client window displays. In this window, there are only two tabs (OSI Model and Outbound PDU Details) because this is the start of the transmission. As more events are examined, there will be three tabs displayed, adding a tab for Inbound PDU Details. When an event is the last event in the stream of traffic, only the OSI Model and Inbound PDU Details tabs are displayed.\nb. Ensure that the OSI Model tab is selected. Under the Out Layers column, ensure that the Layer 7 box is highlighted.\nQuiz : What is the text displayed next to the Layer 7 label? HTTP\nQuiz : What information is listed in the numbered steps directly below the In Layers and Out Layers boxes? The HTTP client sends a HTTP request to the server.\nc. Click Next Layer. Layer 4 should be highlighted. What is the Dst Port value? 80\nd. Click Next Layer. Layer 3 should be highlighted. What is the Dest. IP value? 192.168.1.254\ne. Click Next Layer. ** Quiz :**** What information is displayed at this layer?** Layer 2 Ethernet II Header and inbound and outbound MAC addresses\nf. Click the Outbound PDU Details tab .Information listed under the PDU Details is reflective of the layers within the TCP/IP model. Note: The information listed under the Ethernet II section provides even more detailed information than is listed under Layer 2 on the OSI Model tab. The Outbound PDU Details provides more descriptive and detailed information. The values under DEST MAC and SRC MAC within the Ethernet II section of the PDU Details appear on the OSI Model tab under Layer 2, but are not identified as such.\nQuiz : What information is listed in the numbered steps directly below the In Layers and Out Layers boxes for Layer 7?\nIn Layer: server receives a HTTP request. Out Layer: The server sends back a HTTP reply to the client.\nQuiz : : What is the ** Dst Port ** value for ** Layer 4 ** under the ** Out Layers ** column? Port 1027 Quiz : : What is the ** Dest. IP ** value for ** Layer 3 ** under the ** Out Layers ** column? IP: 192.168.1.1\nQuiz :: What information is displayed at Layer 2 under the Out Layers column? The next-hop IP address is a unicast. The ARP process looks it up in the ARP table. The next-hop IP address is in the ARP table. The ARP process sets the frame's destination MAC address to the one found in the table. The device encapsulates the PDU into an Ethernet frame. g. Click the next colored square box under the Event List \u0026gt; Info column. Only Layer 1 is active (not grayed out). The device is moving the frame from the buffer and placing it on to the network.\nh. Advance to the next HTTP Info box within the Event List and click the colored square box. This window contains both In Layers and Out Layers. Notice the direction of the arrow directly under the In Layers column; it is pointing upward, indicating the direction the information is travelling. Scroll through these layers making note of the items previously viewed. At the top of the column the arrow points to the right. This denotes that the server is now sending the information back to the client.\nQuiz : What is the common information listed under the IP section of PDU Details as compared to the information listed under the OSI Model tab? The Src and Dst Ports, Src and Dst IPs and MAC addresses have been swapped.\nQuiz : With which layer is it associated? Layer 3\n**Quiz : What is the common information listed under the **TCP section of PDU Details, as compared to the information listed under the OSI Model tab, and with which layer is it associated? Src port : 80 and Destination port :1027 . Layer 4.\nQuiz :What is the Host listed under the HTTPsection of the PDU Details? What layer would this information be associated with under the OSI Model tab?\nHTTP Data:Accept-Language: en-us Accept: Layer 7\ni. Click the Outbound PDU Details tab. Scroll down to the HTTP section. Quiz : What is the first line in the HTTP message that displays? HTTP/1.1 200 OK – this means that the request was successful and the page delivered from the server.\nj. Click the last colored square box under the Info column.\nQuiz : How many tabs are displayed with this event and why? Just 2, one for the OSI Model and one for Inbound PDU Details because this is the receiving device\nPart 2: Display Elements of the TCP/IP Protocol Suite #\rIn Part 2 of this activity, you will use the Packet Tracer Simulation mode to view and examine some of the other protocols comprising of the TCP/IP suite.\nStep 1: View Additional Events #\ra. Close any open PDU information windows. b. In the Event List Filters \u0026gt; Visible Events section, click Show All. What additional Event Types are displayed?Depending on whether any communications has occurred prior to starting the original simulation, there should now be entries for ARP, DNS, TCP and HTTP. It is possible that the ARP entries may not show, depending on what a student may have done prior to going to simulation mode. If the activity is started from scratch all of those will be listed. These extra entries play various roles within the TCP/IP suite. If the Address Resolution Protocol (ARP) is listed, it searches MAC addresses. DNS is responsible for converting a name (for example, www.osi.local) to an IP address. The additional TCP events are responsible for connecting, agreeing on communication parameters, and disconnecting the communications sessions between the devices. These protocols have been mentioned previously and will be further discussed as the course progresses. Currently there are over 35 possible protocols (event types) available for capture within Packet Tracer.\nc. Click the first DNS event in the Info column. Explore the OSI Model and PDU Detail tabs and note the encapsulation process. As you look at the OSI Model tab with Layer 7 highlighted, a description of what is occurring is listed directly below the In Layers and Out Layers (“1. The DNS client sends a DNS query to the DNS server.”). This is very useful information to help understand what is occurring during the communication process.\nd. Click the Outbound PDU Details tab.\nQuiz : What information is listed in the NAME : in the DNS QUERY section?\nwww.osi.local e. Click the last DNS Info colored square box in the event list. Quiz : Which device is displayed?The Web ClientWhat is the value listed next to ADDRESS: in the DNS ANSWER section of the Inbound PDU Details?\n192.168.1.254 . This is the address of the Web Server\nf. Find the first HTTP event in the list and click the colored square box of the TCP event immediately following this event. Highlight Layer 4 in the OSI Model tab. In the numbered list directly below the In Layers and Out Layers,\nQuiz : What is the information displayed under items 4 and 5?\nUnder item 4. The TCP connection is successful While Under item 5. The device sets the connection state to \u0026quot;ESTABLISHED\u0026quot;.\nTCP manages the connecting and disconnecting of the communications channel along with other responsibilities. This particular event shows that the communication channel has been ESTABLISHED.g. Click the last TCP event. Highlight Layer 4 in the OSI Model tab. Examine the steps listed directly below In Layers and Out Layers.\nQuiz : What is the purpose of this event, based on the information provided in the last item in the list (should be item 4)?\nThe purpose is CLOSING the connection.\nChallenge #\rThis simulation provided an example of a web session between a client and a server on a local area network (LAN). The client makes requests to specific services running on the server. The server must be set up to listen on specific ports for a client request. (Hint: Look at Layer 4 in the OSI Model tab for port information.)\nQuiz : What port number is the Web Server listening on for the web request? Port 1027\nQuiz : What port is the Web Server listening on for a DNS request? Port 53\nBelow are Images gallery showcasing various visualization in the project Conclusion I gained a clear and practical understanding of the TCP/IP and OSI models. By tracing HTTP requests through each layer, the visualization of data encapsulation and transmission offered an insightful, hands-on approach to networking concepts. The structured breakdown of protocols, ports, and data flow visually presented the critical role that each layer plays in ensuring smooth communication across a network. This exercise is good for anyone seeking to grasp fundamental networking principles .\n","date":"10 February 2025","externalUrl":null,"permalink":"/posts/tcp-ip-and-osi-models-in-action/","section":"Posts","summary":"Step-by-step analysis of OSI model and TCP- IP model using Cisco Packet Tracer. Gain practical knowledge of networking fundamentals while visualizing real-world communication processes.","title":"Visualizing TCP/IP and OSI Models In  Action ","type":"posts"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/arp/","section":"Tags","summary":"","title":"ARP","type":"tags"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/categories/cybersecurity-labs/","section":"Categories","summary":"","title":"Cybersecurity Labs","type":"categories"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/icmp/","section":"Tags","summary":"","title":"ICMP","type":"tags"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/mac-address/","section":"Tags","summary":"","title":"MAC-Address","type":"tags"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/categories/network-troubleshooting/","section":"Categories","summary":"","title":"Network Troubleshooting","type":"categories"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/network-analysis/","section":"Tags","summary":"","title":"Network-Analysis","type":"tags"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/packet-sniffing/","section":"Tags","summary":"","title":"Packet-Sniffing","type":"tags"},{"content":"\rUse Wireshark to View Network Traffic #\rOur task is to utilize wireshark to view Network Traffic flow.\nNetwork Topology #\rObjectives #\rPart 1: Capture and Analyze Local ICMP Data in Wireshark Part 2: Capture and Analyze Remote ICMP Data in Wireshark Background / Scenario #\rWireshark is a software protocol analyzer, or “packet sniffer” application, used for network troubleshooting, analysis, software and protocol development, and education. As data streams travel back and forth over the network, the sniffer “captures” each protocol data unit (PDU) and can decode and analyze its content according to the appropriate RFC or other specifications. Wireshark is a useful tool for anyone working with networks and can be used with most labs in the CCNA courses for data analysis and troubleshooting. In this lab, you will use Wireshark to capture ICMP data packet IP addresses and Ethernet frame MAC addresses.\nInstructions #\rPart 1: Capture and Analyze Local ICMP Data in Wireshark #\rIn Part 1 of this lab, you will ping another PC on the LAN and capture ICMP requests and replies in Wireshark. You will also look inside the frames captured for specific information. This analysis should help to clarify how packet headers are used to transport data to their destination.\nStep 1: Retrieve your PC interface addresses. #\rFor this lab, you will need to retrieve your PC IP address and its network interface card (NIC) physical address, also called the MAC address. a. In a command prompt window, enter ipconfig /all, to the IP address of your PC interface, its description, and its MAC (physical) address.\nC:\\Users\\Student\u0026gt; ipconfig /all Windows IP Configuration Host Name . . . . . . . . . . . . : DESKTOP-NB48BTC Primary Dns Suffix . . . . . . . : Node Type . . . . . . . . . . . . : Hybrid IP Routing Enabled. . . . . . . . : No WINS Proxy Enabled. . . . . . . . : No Ethernet adapter Ethernet: Connection-specific DNS Suffix . : Description . . . . . . . . . . . : Intel(R) 82577LM Gigabit Network Connection Physical Address. . . . . . . . . :00-26-B9-DD-00-91 DHCP Enabled. . . . . . . . . . . : No Autoconfiguration Enabled . . . . : Yes Link-local IPv6 Address . . . . . : fe80::d809:d939:110f:1b7f%20(Preferred) IPv4 Address. . . . . . . . . . . : 192.168.1.147(Preferred) Subnet Mask . . . . . . . . . . . : 255.255.255.0 Default Gateway . . . . . . . . . : 192.168.1.1 \u0026lt;output omitted\u0026gt; b. Ask a team member or team members for their PC IP address and provide your PC IP address to them. Do not provide them with your MAC address at this time.\nStep 2: Start Wireshark and begin capturing data. #\ra. Navigate to Wireshark. Double-click the desired interface to start the packet capture. Make sure the desired interface has traffic. b. Information will start scrolling down the top section in Wireshark. The data lines will appear in different colors based on protocol. This information can scroll by very quickly depending on what communication is taking place between your PC and the LAN. We can apply a filter to make it easier to view and work with the data that is being captured by Wireshark. For this lab, we are only interested in displaying ICMP (ping) PDUs. Type icmp in the Filter box at the top of Wireshark and press Enter, or click the Apply button (arrow sign) to view only ICMP (ping) PDUs. c. This filter causes all data in the top window to disappear, but you are still capturing the traffic on the interface. Navigate to a command prompt window and ping the IP address that you received from your team member. Notice that you start seeing data appear in the top window of Wireshark again. Note: If the PC of your team member does not reply to your pings, this may be because the PC firewall of the team member is blocking these requests. Please see Appendix A: Allowing ICMP Traffic Through a Firewall for information on how to allow ICMP traffic through the firewall using Windows. d. Stop capturing data by clicking the Stop Capture icon.\nStep 3: Examine the captured data. #\rIn Step 3, examine the data that was generated by the ping requests of your team member PC. Wireshark data is displayed in three sections: 1) The top section displays the list of PDU frames captured with a summary of the IP packet information listed; 2) the middle section lists PDU information for the frame selected in the top part of the screen and separates a captured PDU frame by its protocol layers; and 3) the bottom section displays the raw data of each layer. The raw data is displayed in both hexadecimal and decimal form. a. Click the first ICMP request PDU frames in the top section of Wireshark. Notice that the Source column has your PC IP address, and the Destination column contains the IP address of the teammate PC that you pinged. b. With this PDU frame still selected in the top section, navigate to the middle section. Click the plus sign to the left of the Ethernet II row to view the destination and source MAC addresses. Does the source MAC address match your PC interface? Yes Does the destination MAC address in Wireshark match your team member MAC address? Yes How is the MAC address of the pinged PC obtained by your PC? The MAC address is obtained through an ARP request. Note: In the preceding example of a captured ICMP request, ICMP data is encapsulated inside an IPv4 packet PDU (IPv4 header) which is then encapsulated in an Ethernet II frame PDU (Ethernet II header) for transmission on the LAN.\nPart 2: Capture and Analyze Remote ICMP Data in Wireshark #\rIn Part 2, you will ping remote hosts (hosts not on the LAN) and examine the generated data from those pings. You will then determine what is different about this data from the data examined in Part 1.\nStep 1: Start capturing data on the interface. #\ra. Start the data capture again. b. A window prompts you to save the previously captured data before starting another capture. It is not necessary to save this data. Click Continue without Saving. c. With the capture active, ping the following three website URLs from a Windows command prompt:\nwww.yahoo.com2) www.cisco.com3) www.google.com Note: When you ping the URLs listed, notice that the Domain Name Server (DNS) translates the URL to an IP address. Note the IP address received for each URL. d. You can stop capturing data by clicking the Stop Capture icon. Step 2: Examining and analyzing the data from the remote hosts. #\rReview the captured data in Wireshark and examine the IP and MAC addresses of the three locations that you pinged. List the destination IP and MAC addresses for all three locations in the space provided. IP address for www.yahoo.com: MAC address for www.yahoo.com: IP address for www.cisco.com: MAC address for www.cisco.com: IP address for www.google.com: MAC address for www.google.com: IP addresses: 98.137.246.7, 96.7.79.147, 172.217.14.100 (these IP addresses may vary) AC address: This will be the same for all three locations. It is the physical address of the default-gateway LAN interface of the router. What is significant about this information? The MAC addresses for all three locations are the same. How does this information differ from the local ping information you received in Part 1? A ping to a local host returns the MAC address of the PC NIC. A ping to a remote host returns the MAC address of the default gateway LAN interface.\nReflection Question #\rWhy does Wireshark show the actual MAC address of the local hosts, but not the actual MAC address for the remote hosts? MAC addresses for remote hosts are not known on the local network, so the MAC address of the default-gateway is used. After the packet reaches the default-gateway router, the Layer 2 information is stripped from the packet and a new Layer 2 header is attached with the destination MAC address of the next hop router. Conclusion #\rWireshark shows the activity and requests in a network. In some seconds interval so many requests and communication may happen in a network. Through the command line, one can also obtain some information about a network.\nAcknowledgement: This lab is part of Cisco Netacad academy labs on Networking.\n","date":"8 February 2025","externalUrl":null,"permalink":"/posts/wireshark-to-view-network-traffic/use_wireshark_to_view_network_traffic/","section":"Posts","summary":"Step-by-step tutorial on using Wireshark for network analysis. Capture ICMP packets, compare local and remote traffic behavior, and explore Layer 2/Layer 3 addressing in practical scenarios.","title":"Use Wireshark to View Network Traffic","type":"posts"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/wireshark/","section":"Tags","summary":"","title":"Wireshark","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/compliance/","section":"Tags","summary":"","title":"Compliance","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/cybersecurity/","section":"Tags","summary":"","title":"Cybersecurity","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/categories/cybersecurity--compliance/","section":"Categories","summary":"","title":"Cybersecurity \u0026 Compliance","type":"categories"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/categories/data-privacy-laws/","section":"Categories","summary":"","title":"Data Privacy Laws","type":"categories"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/data-protection/","section":"Tags","summary":"","title":"Data-Protection","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/financial-regulations/","section":"Tags","summary":"","title":"Financial-Regulations","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/gdpr/","section":"Tags","summary":"","title":"GDPR","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/healthcare-compliance/","section":"Tags","summary":"","title":"Healthcare-Compliance","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/hipaa/","section":"Tags","summary":"","title":"HIPAA","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/kenya-dpa/","section":"Tags","summary":"","title":"Kenya-DPA","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/legal-frameworks/","section":"Tags","summary":"","title":"Legal-Frameworks","type":"tags"},{"content":"\rNavigating Global Data Protection Compliance: Frameworks, Regulations, and Implementation Strategies #\rIntroduction #\rIn the recent weeks there has been some data breaches. To ensure compliance there are various regulatory compliance that ensure data is protected . Part of cybersecurity assessment service involves evaluating a client’s adherence to relevant compliance frameworks. This is no small responsibility, as failure to meet compliance requirements can result in heavy penalties. In some cases, our client might even be banned from doing business in certain countries due to non-compliance.\nTake our client ABC , for example. They collect customer information as part of their business operations in Kenya, the EU, and the US. Additionally, they handle credit card transactions for purchases made through their online storefront. This means that client ABC must comply with frameworks such as GDPR and PCI DSS, among others.\nWhile we’re on the topic of legal considerations, it’s important to familiarize ourselves with features of our customer agreements. These include contracts, service-level agreements (SLAs), confidentiality requirements, master service agreements (MSAs), and statements of work (SOWs). Understanding these agreements and how they affect our practice is essential for delivering high-quality services. Before performing any audit we ensure that all the legal compliance are in place. This is to prtoect us and the client.\nRegulatory Compliance Considerations #\rTo succeed in ethical hacking and penetration testing, you must be familiar with several regulatory compliance considerations. This knowledge is not only necessary for completing compliance-based assessments but also for understanding the regulations that may impact both you and your client.\nLet’s start by assuming that you’ve been hired to perform a compliance-based assessment. In this scenario, you, as the penetration tester, are responsible for verifying and auditing the security posture of the organization. Your goal is to ensure that the organization complies with specific regulations, such as the following:\nPCI DSS #\rThe Payment Card Industry Data Security Standard (PCI DSS) regulation aims to secure the processing of credit card payments and other types of digital payments. You can access PCI DSS specifications, documentation, and resources at pcisecuritystandards.org.\nHIPAA #\rThe Health Insurance Portability and Accountability Act of 1996 (HIPAA) was originally designed to simplify and standardize healthcare administrative processes. This involved transitioning from paper records and transactions to electronic records and transactions. The U.S. Department of Health and Human Services (HHS) was tasked with developing and publishing standards to protect an individual’s electronic health information while allowing appropriate access and use by healthcare providers and other entities. For more information about HIPAA, visit https://www.cdc.gov/phlp/publications/topic/hipaa.html.\nOften, these regulations require companies to hire third-party penetration testing firms to ensure compliance and security. Familiarity with these regulations is key if you\u0026rsquo;re performing penetration tests to verify compliance and overall security. Many standards provide checklists for assessments.\nYou should also become familiar with privacy-related regulations, such as the General Data Protection Regulation (GDPR). GDPR includes strict rules around data processing and privacy. One of its primary goals is to strengthen and unify data protection for individuals within the European Union (EU), while addressing the export of personal data outside the EU. Essentially, the main objective of GDPR is to give citizens control over their personal data. Additional information about GDPR can be found at gdpr-info.eu .\nTo effectively complete a compliance-based assessment, you should familiarize yourself with some of the key underlying regulations, such as those described in the following sections.\nRegulations in the Financial Sector #\r💰\nInternational context (USA) Financial services institutions, such as banks, credit unions, and lending institutions, offer a wide range of solutions and financial instruments. While you might think that money is their most valuable asset, in reality, customer and transactional information is the heart of their business. Financial assets are material and can be replaced, but protecting customer information is crucial for establishing and maintaining trust between a financial institution and the community it serves. More specifically, institutions have a responsibility to safeguard the privacy of individual consumers and protect them from harm, including fraud and identity theft. On a broader scale, the industry is responsible for maintaining the critical infrastructure of the nation’s financial services.\nHere are a few examples of regulations applicable to the financial sector:\nTitle V, Section 501(b) of the Gramm-Leach-Bliley Act (GLBA) and the corresponding interagency guidelines The Federal Financial Institutions Examination Council (FFIEC) The Federal Deposit Insurance Corporation (FDIC) Safeguards Act and Financial Institutions Letters (FILs) Compliance with some regulations, such GLBA, is mandatory.\nGLBA defines a financial institution as “any institution the business of which is significantly engaged in financial activities . GLBA applies to all financial services organizations, regardless of size. This definition is important because it includes many companies that are not traditionally considered financial institutions. The law also applies to companies that receive information about customers of other financial institutions, including credit reporting agencies and ATM operators.\nRegulations in the Financial Sector #\r💰 Kenyan context\nIn the Kenyan context, there are specific regulations that ensure data protection, privacy, and operational integrity. In the financial sector, key regulations include the Central Bank of Kenya (CBK) Prudential Guidelines, which mandate cybersecurity measures and risk management frameworks for banks and financial institutions. Additionally, the Data Protection Act (2019) aligns with global standards like GDPR, requiring financial institutions to safeguard customer data and ensure compliance with data privacy principles. Non-compliance can result in hefty penalties or restrictions on operations. Another critical regulation is the Capital Markets Authority (CMA) Cybersecurity Guidelines, which focus on protecting digital transactions and investor information in capital markets. These frameworks collectively emphasize the need for robust penetration testing, vulnerability assessments, and adherence to security standards.\nRegulations in the Healthcare Sector #\r🩺\nIn the healthcare sector, the Kenya Health Information System Policy and the Data Protection Act (2019) play pivotal roles in safeguarding patient information and ensuring secure electronic health records (EHR). Healthcare providers must adhere to strict confidentiality requirements when handling personally identifiable health information (PIIHI), similar to HIPAA in the U.S. but tailored to local needs. Furthermore, the Health Act (2017) mandates the protection of patient rights, including data privacy, and requires healthcare facilities to implement technical and administrative safeguards. Compliance with these regulations is crucial for maintaining trust and avoiding legal repercussions. Together, these frameworks highlight the importance of securing sensitive data and ensuring the healthcare organizations in Kenya maintain a strong security posture to mitigate risks .\nPayment Card Industry Data Security Standard (PCI DSS) #\rTo protect cardholders against misuse of their personal information and minimize payment card channel losses, the major payment card brands (Visa, MasterCard, Discover, and American Express) formed the Payment Card Industry Security Standards Council (PCI SSC) and developed the Payment Card Industry Data Security Standard (PCI DSS). The latest version of the standard and collateral documentation can be obtained at pcisecuritystandards.org .\nPCI DSS must be adopted by any organization that transmits, processes, or stores payment card data or that directly or indirectly affects the security of cardholder data. Any organization that leverages a third party to manage cardholder data has the full responsibility of ensuring that this third party is compliant with PCI DSS. The payment card brands can levy fines and penalties against organizations that do not comply with the requirements and/or can revoke their authorization to accept payment cards.\nBefore we proceed with details about how to protect cardholder data and guidance on how to perform penetration testing in PCI environments, we must define several key terms that are used in this module and are defined by the PCI SSC at pcisecuritystandards.org/documents/PCI_DSS_Glossary_v3-2.pdf. Let’s explore each term:\nAcquirer Also referred to as an “acquiring bank” or an “acquiring financial institution,” an entity that initiates and maintains relationships with merchants for the acceptance of payment cards. ASV (Approved Scanning Vendor) An organization approved by the PCI SSC to conduct external vulnerability scanning services.\nMerchant For the purposes of PCI DSS, an entity that accepts payment cards bearing the logos of any of the members of PCI SSC (American Express, Discover, MasterCard, or Visa) as payment for goods and/or services. Note that a merchant that accepts payment cards for goods and/or services can also be a service provider if the services sold result in storing, processing, or transmitting cardholder data on behalf of other merchants or service providers. For example, an ISP is a merchant that accepts payment cards for monthly service. PAN (Primary Account Number) A payment card number that is up to 19 digits long.\nPayment Brand Brands such as Visa, MasterCard, Amex, or Discover.\nPCI Forensic Investigator (PFI) A person trained and certified to investigate and contain information about cybersecurity incidents and breaches involving cardholder data.\nQualified Security Assessor (QSA) An individual trained and certified to carry out PCI DSS compliance assessments.\nService Provider A business entity that is not a payment brand and that is directly involved in the processing, storage, or transmission of cardholder data. This includes companies that provide services that control or could impact the security of cardholder data, such as managed service providers that provide managed firewalls, intrusion detection, and other services, and hosting providers and other entities. Entities such as telecommunications companies that only provide communication links without access to the application layer of the communication link are excluded.\nTo counter the potential for staggering losses, the payment card brands contractually require that all organizations that store, process, or transmit cardholder data and/or sensitive authentication data comply with PCI DSS. PCI DSS requirements apply to all system components where account data is stored, processed, or transmitted.\nAccount data consists of cardholder data as well as sensitive authentication data. A system component is any network component, server, or application that is included in, or connected to, the cardholder data environment. The cardholder data environment is defined as the people, processes, and technology that handle cardholder data or sensitive authentication data.\nThe PAN is the defining factor in the applicability of PCI DSS requirements. PCI DSS requirements apply if the PAN is stored, processed, or transmitted. If the PAN is not stored, processed, or transmitted, PCI DSS requirements do not apply. If cardholder name, service code, and/or expiration date are stored, processed, or transmitted with the PAN or are otherwise present in the cardholder data environment, they too must be protected. Per the standards, the PAN must be stored in an unreadable (encrypted) format. Sensitive authentication data may never be stored post-authorization, even if encrypted.\nThe PCI SSC website provides great guidance on the requirements for penetration testing. See pcisecuritystandards.org.\nKey Technical Elements in Regulations to Consider #\rMost regulations dictate several key elements, and a penetration tester should pay attention to and verify them during assessments to ensure the organization is compliant. Let’s explore each element:\nData Isolation #\rOrganizations that need to comply with PCI DSS (and other regulations, for that matter) should have a data isolation strategy. Also known as network isolation or network segmentation, the goal is to implement a completely isolated network that includes all systems involved in payment card processing.\nPassword Management #\rMost regulations mandate solid password management strategies. For example, organizations must not use vendor-supplied defaults for system passwords and security parameters. This requirement also extends far beyond its title and enters the realm of configuration management. In addition, most of these regulations mandate specific implementation standards, including password length, password complexity, and session timeout, as well as the use of multifactor authentication.\nKey Management #\rWhere data is encrypted ,the cryptographic key needs good management. Key management policy and standards should include assigned responsibility for key management, the nature of information to be protected, the classes of threats, the cryptographic protection mechanisms to be used, and the protection requirements for the key and associated processes. to get more guidanceon this you can consider visiting https://csrc.nist.gov/projects/key-management/key-management-guidelines\nTIP: Your customer might have specific corporate policies that need to be taken into consideration when performing a penetration test. In most cases, the customer will initially disclose in its corporate policy any items that might have a direct impact on the penetration testing engagement, but you should always ask and clearly document whether there are any. Some companies might also be under specific regulations that require them to create vulnerability and penetration testing policies. These regulations might specify restricted and nonrestricted systems and information on how a penetration test should be conducted according to a regulatory standard.\nDuring your pre-engagement tasks, you should identify testing constraints, including tool restrictions. Often, you will be constrained by certain aspects of the business and the technology in the organization . In addition, the following are a few examples of constraints that you might face during an audit or penetration testing engagement:\nCertain areas and technologies that cannot be tested due to operational limitations. Technologies that might be specific to the organization being tested Limitation of known exploits Systems that are categorized as out of scope because of their criticality or known performance problems You should clearly communicate any technical constraints with the appropriate stakeholders of the organization that hired you prior to and during the testing.\nLegal Scope during audit and pentesting. #\rThe following are several important legal concepts that you must know when performing a penetration test. Let’s explore each one:\nService-Level Agreement (SLA) #\rAn SLA is a well-documented expectation or constraint related to one or more of the minimum and/or maximum performance measures (such as quality, timeline/timeframe, and cost) of the penetration testing service. You should become familiar with any SLAs that the organization that hired you has provided to its customers.\nConfidentiality #\rYou must discuss and agree on the handling of confidential data. For example, if you are able to find passwords or other sensitive data, do you need to disclose all those passwords or all that sensitive data? Who will have access to the sensitive data? What will be the proper way to communicate and handle such data? Similarly, you must protect sensitive data and delete all records, per your agreement with your client. Your customer could have specific data retention policies that you might also have to be aware of. Every time you finish a penetration testing engagement, you should delete any records from your systems. You do not want your next customer to find sensitive information from another client in any system or communication.\nStatement of Work (SOW) #\rAn SOW is a document that specifies the activities to be performed during a penetration testing engagement. It can be used to define some of the following elements:\nProject (penetration testing) timelines, including the report delivery schedule The scope of the work to be performed The location of the work (geographic location or network location) Special technical and nontechnical requirements Payment schedule Miscellaneous items that may not be part of the main negotiation but that need to be listed and tracked because they could pose problems during the overall engagement The SOW can be a standalone document or can be part of a master service agreement (MSA).\nMaster Service Agreement (MSA) #\rMSAs, which are very popular today, are contracts that can be used to quickly negotiate the work to be performed. When a master agreement is in place, the same terms do not have to be renegotiated every time you perform work for a customer. MSAs are especially beneficial when you perform a penetration test and know that you will be rehired on a recurring basis to perform additional tests in other areas of the company or to verify that the security posture of the organization has been improved as a result of prior testing and remediation.\nNon-Disclosure Agreement (NDA) #\rAn NDA is a legal document and contract between you and an organization that has hired you as a penetration tester. An NDA specifies and defines confidential material, knowledge, and information that should not be disclosed and that should be kept confidential by both parties. NDAs can be classified as any of the following:\nUnilateral: With a unilateral NDA, only one party discloses certain information to the other party, and the information must be kept protected and not disclosed. For example, an organization that hires you should include in an NDA certain information that you should not disclose. Of course, all of your findings must be kept secret and should not be disclosed to any other organization or individual. Bilateral: A bilateral NDA is also referred to as a mutual, or two-way, NDA. In a bilateral NDA, both parties share sensitive information with each other, and this information should not be disclosed to any other entity. Multilateral: This type of NDA involves three or more parties, with at least one of the parties disclosing sensitive information that should not be disclosed to any entity outside the agreement. Multilateral NDAs are used in the event that an organization external to your customer (business partner, service provider, and so on) should also be engaged in the penetration testing engagement. Conclusion #\rBy incorporating these best practices, organizations and penetration testers can ensure regulatory compliance while maintaining a clear understanding of their legal obligations. As we consider Kenya\u0026rsquo;s Data Protection Act ,2019 ,having a globalperspective ensures we maintain and adopt the best practise when data is concerned.\n","date":"6 February 2025","externalUrl":null,"permalink":"/posts/data-protection/","section":"Posts","summary":"Explore essential data protection regulations including GDPR, HIPAA, PCI DSS, and Kenya\u0026rsquo;s Data Protection Act. Learn compliance strategies, sector-specific requirements, and legal considerations for penetration testing and audits.","title":"Navigating Global Data Protection Compliance: Frameworks, Regulations, and Implementation Strategies","type":"posts"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/pci-dss/","section":"Tags","summary":"","title":"PCI-DSS","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/categories/professional-certification/","section":"Categories","summary":"","title":"Professional Certification","type":"categories"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/access-control/","section":"Tags","summary":"","title":"Access-Control","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/categories/aws/","section":"Categories","summary":"","title":"AWS","type":"categories"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/aws-cli/","section":"Tags","summary":"","title":"AWS-CLI","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/aws-iam/","section":"Tags","summary":"","title":"AWS-IAM","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/categories/cloud-security/","section":"Categories","summary":"","title":"Cloud Security","type":"categories"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/cloud-security/","section":"Tags","summary":"","title":"Cloud-Security","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/cloudgoat/","section":"Tags","summary":"","title":"CloudGoat","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/categories/cybersecurity-training/","section":"Categories","summary":"","title":"Cybersecurity Training","type":"categories"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/categories/ethical-hacking/","section":"Categories","summary":"","title":"Ethical Hacking","type":"categories"},{"content":"\rIntroduction #\rCloud Identity and Access Management (Cloud IAM) is a framework for managing digital identities and controlling access to resources in cloud environments. It involves verifying user identities (authentication), controlling user actions and resource access (authorization), and assigning permissions based on roles (role-based access control). Cloud IAM also includes defining and enforcing access policies (policy management), tracking user activities for security and compliance (auditing and monitoring), integrating with external identity providers (federation), allowing one login for multiple services (single sign-on), and ensuring adherence to regulatory and security standards (compliance and governance). Popular solutions for Cloud IAM include AWS IAM, Google Cloud IAM, and Microsoft Entra ID (formerly Azure Active Directory) CloudGoat developed by Rhino Security Labs, is a tool designed to intentionally deploy vulnerable AWS environments for the purpose of practicing cloud cybersecurity skills. It sets up various \u0026ldquo;capture-the-flag\u0026rdquo; scenarios that simulate real-world security challenges. One scenario provided by CloudGoat is \u0026ldquo;iam_privesc_by_rollback,\u0026rdquo; which focuses on demonstrating how privilege escalation can occur in AWS through the rollback of IAM policies. This scenario allows users to explore and understand the risks associated with IAM policy rollbacks and to practice mitigating such vulnerabilities.\nObjective: #\rTo understand and practice privilege escalation techniques in AWS IAM (Identity and Access Management) using the CloudGoat scenario \u0026ldquo;iam_privesc_by_rollback.\u0026rdquo;\nInstall and Configure the AWS CLI in the local environment:\nNavigate over to https://aws.amazon.com/cli/ . We follow the steps to install the AWS CLI on Linux. To verify a successful install, we check the $aws –version .\nOur output shows the versions , indicating the AWS cli is installed and configured.\nWe navigate to AWS IAM service and create a user , in this case our user is “kingFace”. Next we configure the policy and assign AdministratorAccess to our user kingFace. The user is successfully created. Thus we need to create access keys to Access the profile from the Terminal. #\rTo create access keys we go to security credentials and proceed to create the access keys.\nWe select the cli use case.\nWe then create a description followed by retrieving the access keys.\rAs we create access keys we are notified of best practices for managing AWS access keys. They are essential for security and risk reduction. These include using IAM roles instead of access keys to reduce exposure, deleting unused keys, and regularly rotating them to limit potential unauthorized access. Applying the principle of least privilege minimizes damage if credentials are misused. Monitoring and auditing regularly to detect suspicious activities, while avoiding hard-coding keys prevents accidental exposure. Encrypting sensitive data, enabling multi-\nfactor authentication (MFA), and using condition keys in IAM policies add layers of protection.\nRegular reviews of IAM users, roles, and policies ensure compliance and address vulnerabilities. Exposed access keys can give access to malicious attackers to get into the cloud environment.\nWe can verify connecting to our profile by the provided access keys, by running aws s3 ls to list available s3 buckets. From the image above we have one bucket “jamesmwangi’.\rNext we install the required dependencies and software to run cloudgoat.\rWe run into an error while trying to configure a profile as recommended.\nAnd yet verifying if Terraform was installed by checking the version, indicated the software was installed. As a way to trouble shoot , opened the cloudgoat.py code in cloudgoat.py line 103 we can see that if the version requirement is unmet, the error will indicate Terraform not found. Same error as when Terraform is not installed.\nWe can update to the latest version using tfenv CLI tool. After updating to the latest version the $ ./cloudgoat.py config profile** finally runs without errors. This troubleshooting exercise revealed the principle of checking the related configuration scripts and documentation before trying out solutions from Google and forums.\nWhen prompted to create a default profile we choose yes, then we configure the profile as Kface. We use the access keys we obtained earlier.\nWe configure the profile and whitelist our IP address. The steps to the scenario:\rhttps://github.com/RhinoSecurityLabs/cloudgoat/tree/master/scenarios/iam_privesc_by_rollback\nWe start by running ./cloudgoat.py create iam_privesc_by_rollback. From the output we can get credentials to set up Raynor profile.\nTo configure the profile we use :aws configure \u0026ndash;profile Raynor\nWe also run :aws –profile Raynor sts get-caller-identity to obtain the username from the arn value, which we’ll use later.\n*$aws iam list-attached-user-policies \u0026ndash;user-name raynor-iam_privesc_by_rollback_cgid53i9v2ug4p \u0026ndash;profile Raynor *The above command lists the policies associated with the Raynor user.\nThe displayed policy includes the policy name and the Amazon Resource Name (ARN) Next we run : $aws iam list-policy-versions \u0026ndash;policy-arn arn:aws:iam::891377237476:policy/cg-raynor-policy- iam_privesc_by_rollback_cgid53i9v2ug4p \u0026ndash;profile Raynor\nWe wish to find the available versions entailed in the policy versions. The default version is v1. That is the version Raynor has access to. V1:\nIt has 3 allowed actions. With Get, List, and SetDefaultPolicyVersion permissions, an attacker could exploit the SetDefaultPolicyVersion permission to switch to an older policy version that grants full admin rights. They would identify which version offers full admin rights and then set it as the default.\nWe can test if v1, the default version can allow us to run other commands other than the 3 allowed . We get an error , “UnauthorizedOperation”.\rV2:\rv2 allows access to the Buckets and all resources. Concerning s3 buckets, one can List ,Get and list all buckets.\rV3:\nv3 has administrator access allowing all actions. We’ll try to switch to this version later to run any command. With such access the attacker can be able to execute any instruction on our cloud environment.\rV4:\nv4 introduces the concept of timed access. Access to the profile expires after the time duration. To enhance security, this ensures we allow privileged access only when needed. The profile access will expire after a set duration, and it allows only the Get action.\nV5:\rv5 allows administrative access exclusively from a previously whitelisted private IP address while blocking all non-whitelisted IP addresses from accessing any actions.\nNext we can now set the default policy version to v3. We can also verify the change . From the output we observe that v3 is now the default version.\nNow we can test if Raynor can run the describe instances operation.\rThe operation runs unrestricted. An attacker can potentially gain full admin privileges and execute any malicious actions they desire, as demonstrated in this exercise.\nOne way to monitor policies on AWS is CloudTrail. Monitoring and auditing API calls with CloudTrail help detect suspicious activities and respond to security incidents promptly.\nConclusion #\rThe CloudGoat IAM exercise showcases critical security vulnerabilities within IAM configurations. The exercise brings out the need to have robust monitoring , access control and policy management. To mitigate the risk of privileged escalation, carefully control who has the SetDefaultPolicyVersion permission. Regularly review policy versions and their permissions. Remove or deactivate any outdated or insecure policy versions. The exercise serves as a valuable lesson in securing IAM configurations to safeguard against potential breaches and maintain the integrity of cloud environments.\n","date":"5 February 2025","externalUrl":null,"permalink":"/posts/iam-priviledge-escallation/","section":"Posts","summary":"Explore how attackers exploit IAM policy rollbacks to gain administrative access in AWS environments. This CloudGoat scenario demonstrates privilege escalation techniques, policy version analysis, and critical cloud security hardening practices.","title":"IAM Privilege Escalation via Policy Rollback","type":"posts"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/policy-management/","section":"Tags","summary":"","title":"Policy-Management","type":"tags"},{"content":"","date":"5 February 2025","externalUrl":null,"permalink":"/tags/privilege-escalation/","section":"Tags","summary":"","title":"Privilege-Escalation","type":"tags"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/categories/azure-labs/","section":"Categories","summary":"","title":"Azure Labs","type":"categories"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/azure-firewall/","section":"Tags","summary":"","title":"Azure-Firewall","type":"tags"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/cloud-infrastructure/","section":"Tags","summary":"","title":"Cloud-Infrastructure","type":"tags"},{"content":"Introduction\nIn LAB_03_AzureFirewall, we will learn how to deploy and configure Azure Firewall, a critical component for securing an Azure Virtual Network. Azure Firewall is a managed cloud-based network security service designed to protect Azure resources by controlling and monitoring network traffic. Throughout this lab, we will gain practical experience in setting up Azure Firewall. By the end of this lab, we will have a solid understanding of how to implement and manage Azure Firewall to enhance the security and compliance of Azure infrastructure.\nLab scenario\nYou have been asked to install Azure Firewall. This will help your organization control inbound and outbound network access which is an important part of an overall network security plan. Specifically, you would like to create and test the following infrastructure components:\nA virtual network with a workload subnet and a jump host subnet. A virtual machine is each subnet. A custom route that ensures all outbound workload traffic from the workload subnet must use the firewall. Firewall Application rules that only allow outbound traffic to www.bing.com. Firewall Network rules that allow external DNS server lookups. For all the resources in this lab, we are using the East US region. Verify with your instructor this is the region to use for class.\nLab objectives\nIn this lab, you will complete the following exercise:\nExercise 1: Deploy and test an Azure Firewall Azure Firewall diagram\nExercise 1: Deploy and test an Azure Firewall\nEstimated timing: 40 minutes\nFor all the resources in this lab, we are using the East Asia region.\nTask 1: Use a template to deploy the lab environment. Task 2: Deploy an Azure firewall. Task 3: Create a default route. Task 4: Configure an application rule. Task 5: Configure a network rule. Task 6: Configure DNS servers. Task 7: Test the firewall. Task 1: Use a template to deploy the lab environment.\nIn this task, you will review and deploy the lab environment.\nIn this task, you will create a virtual machine by using an ARM template. This virtual machine will be used in the last exercise for this lab.\nSign-in to the Azure portal https://portal.azure.com/.\nNote: Sign in to the Azure portal using an account that has the Owner or Contributor role in the Azure subscription you are using for this lab.\nIn the Azure portal, in the Search resources, services, and docs text box at the top of the Azure portal page, type Deploy a custom template and press the Enter key.\nOn the Custom deployment blade, click the Build your own template in the editor option.\nOn the Edit template blade, click Load file, locate the \\Allfiles\\Labs\\08\\template.json file and click Open.\nNote: Review the content of the template and note that it deploys an Azure VM hosting Windows Server 2016 Datacenter.\nOn the Edit template blade, click Save.\nHad to edit the template from eastus to eastasia due to errors in Azure Regions resources allowed. On the template file click Ctrl+F to find and replace easily.\nOn the Custom deployment blade, ensure that the following settings are configured (leave any others with their default values):\nSetting Value\nSubscription the name of the Azure subscription\nyou will be using in this lab\nResource group click Create new and type the\nname AZ500LAB08\nLocation (US) East US\nSetting Value\nadminPassword A secure password of your own\nchoosing for the virtual machines. Remember the password. You will need it later to connect to the VMs.\nNote: To identify Azure regions where you can provision Azure VMs, refer to https://azure.microsoft.com/en-us/regions/offers/\nClick Review + create, and then click Create.\nNote: Wait for the deployment to complete. This should take about 2 minutes.\nTask 2: Deploy the Azure firewall\nIn this task you will deploy the Azure firewall into the virtual network.\nIn the Azure portal, in the Search resources, services, and docs text box at the top of the Azure portal page, type Firewalls and press the Enter key. On the Firewalls blade, click + Create. On the Basics tab of the Create a firewall blade, specify the following settings (leave others with their default values): 4\nSetting\nResource group Name\nRegion\nFirewall SKU Firewall management\nChoose a virtual network Public IP address\n\u000eValue AZ500LAB08 Test-FW01 (US) East US Standard\nUse Firewall rules (classic) to manage this firewall\nclick the Use existing option and, in the drop-down list, select Test-FW-VN\nclck Add new and type the name TEST-FW-PIP and click OK\nClick Review + create and then click Create.\nNote: Wait for the deployment to complete. This should take about 5 minutes.\nIn the Azure portal, in the Search resources, services, and docs text box at the top of the Azure portal page, type Resource groups and press the Enter key.\nOn the Resource groups blade, in the list of resource group, click the AZ500LAB08 entry.\nNote: On the AZ500LAB08 resource group blade, review the list of resources. You can sort by Type.\nIn the list of resources, click the entry representing the Test-FW01 firewall.\nOn the Test-FW01 blade, identify the Private IP address that was assigned to the firewall. The private IP is 10.0.1.4, highlighted in blue in the image below\nNote: You will need this information in the next task.\rTask 3: Create a default route\nIn this task, you will create a default route for the Workload-SN subnet. This route will configure outbound traffic through the firewall.\nIn the Azure portal, in the Search resources, services, and docs text box at the top of the Azure portal page, type Route tables and press the Enter key.\nOn the Route tables blade, click + Create.\nOn the Create route table blade, specify the following settings:\nSetting Value Resource group AZ500LAB08 Region East Asia Name Firewall-route\nClick Review + create, then click Create, and wait for the provisioning to complete. ￼\nOn the Route tables blade, click Refresh, and, in the list of route tables, click the Firewall-route entry.\nOn the Firewall-route blade, in the Settings section, click Subnets and then, on the Firewall-route | Subnets blade, click + Associate.\nOn the Associate subnet blade, specify the following settings:\nSetting Value Virtual network Test-FW-VN\nSetting Value\nSubnet Workload-SN\nNote: Ensure the Workload-SN subnet is selected for this route, otherwise the firewall won’t work correctly.\nClick OK to associate the firewall to the virtual network subnet.\nBack on the Firewall-route blade, in the Settings section, click Routes and then click + Add.\nOn the Add route blade, specify the following settings:\n8\nSetting\nRoute name\nAddress prefix destination Destination IP addresses/CIDR ranges Next hop type\nNext hop address\n\u000eValue\nFW-DG\nIP Address 0.0.0.0/0\nVirtual appliance\nthe private IP address of the firewall that you identified in the previous task\nNote: Azure Firewall is actually a managed service, but virtual appliance works in this situation\nClick Add to add the route. Task 4: Configure an application rule\nIn this task you will create an application rule that allows outbound access to www.bing.com.\nIn the Azure portal, navigate back to the Test-FW01 firewall.\nOn the Test-FW01 blade, in the Settings section, click Rules (classic).\nOn the Test-FW01 | Rules (classic) blade, click the Application rule collection tab, and then click + Add application rule collection.\nOn the Add application rule collection blade, specify the following settings (leave others with their default values):\nSetting Value Name App-Coll01 Priority 200\nAction Allow\nOn the Add application rule collection blade, create a new entry in the Target FQDNs section with the following settings (leave others with their default values):\nSetting Value\nname AllowGH Source type IP Address Source 10.0.2.0/24\nProtocol port http:80, https:443 Target FQDNS www.bing.com\nClick Add to add the Target FQDNs-based application rule.\nNote: Azure Firewall includes a built-in rule collection for infrastructure FQDNs that are allowed by default. These FQDNs are specific for the platform and can’t be used for other purposes.\nTask 5: Configure a network rule\nIn this task, you will create a network rule that allows outbound access to two IP addresses on port 53 (DNS).\nIn the Azure portal, navigate back to the Test-FW01 | Rules (classic) blade.\nOn the Test-FW01 | Rules (classic) blade, click the Network rule collection tab and then click + Add network rule collection.\nOn the Add network rule collection blade, specify the following settings (leave others with their default values):\nSetting Value Name Net-Coll01 Priority 200\nAction Allow\nOn the Add network rule collection blade, create a new entry in the IP Addresses section with the following settings (leave others with their default values):\n11\nSetting\nName\nProtocol\nSource type\nSource Addresses\nDestination type\nDestination Address\nDestination Ports\nClick Add to add the network rule. \u000eValue\nAllowDNS\nUDP\nIP address\n10.0.2.0/24\nIP address 209.244.0.3,209.244.0.4 53\nNote: The destination addresses used in this case are known public DNS servers.\rTask 6: Configure the virtual machine DNS servers\nIn this task, you will configure the primary and secondary DNS addresses for the virtual machine. This is not a firewall requirement.\nIn the Azure portal, navigate back to the AZ500LAB08 resource group.\nOn the AZ500LAB08 blade, in the list of resources, click the Srv-Work virtual machine.\nOn the Srv-Work blade, in the Settings section, click Networking.\nOn the Srv-Work | Networking blade, click the link next to the Network interface entry.\nOn the network interface blade, in the Settings section, click DNS servers, select the Custom option, add the two DNS servers referenced in the network rule: 209.244.0.3 and 209.244.0.4, and click Save to save the change.\nWhile adding a server , there was an error that the IP addresses were not valid.\rThe error is as a result of configuring the DNS server in the previous task as 209.244.0.3,209.244.0.4\nAdding a spacing after the comma allows for the IP input to be read accordingly by the firewall\nAfter fixing the Destination address, the changes are saved.\rReturn to the Srv-Work virtual machine page. Note: Wait for the update to complete.\nNote: Updating the DNS servers for a network interface will automatically restart the virtual machine to which that interface is attached, and if applicable, any other virtual machines in the same availability set.\nTask 7: Test the firewall\nIn this task, you will test the firewall to confirm that it works as expected.\nIn the Azure portal, navigate back to the AZ500LAB08 resource group.\nOn the AZ500LAB08 blade, in the list of resources, click the Srv-Jump virtual machine.\nOn the Srv-Jump blade, click Connect and, in the drop down menu, click RDP.\rClick Download RDP File and use it to connect to the Srv-Jump Azure VM via Remote Desktop. When prompted to authenticate, provide the following credentials:\nSetting Value\nUser name localadmin\nPassword The secure password you chose\nduring deployment of the custom template in task 1 step 6.\nIn Linux the remote desktop client Remmima , fill in the username and password field.\rFor the server field input the server address in the rdp file downloaded 20.189.98.221:3389\nNote: The following steps are performed in the Remote Desktop session to the Srv-Jump Azure VM.\nNote: You will connect to the Srv-Work virtual machine. This is being done so we can test the ability to access the bing.com website.\nWithin the Remote Desktop session to Srv-Work, in Server Manager, click Local Server and then click IE Enhanced Security Configuration.\nIn the Internet Explorer Enhanced Security Configuration dialog box, set both options to Off and click OK.\nWithin the Remote Desktop session to Srv-Work, start Internet Explorer and browse to https://www.bing.com.\nThe connection to bing.com goes through\nNote: The website should successfully display. The firewall allows you access.\nBrowse to http://www.microsoft.com/\nNote: Within the browser page, you should receive a message with text resembling the following: HTTP request from 10.0.2.4:xxxxx to microsoft.com:80. Action: Deny. No rule matched. Proceeding with default action. This is expected, since the firewall blocks access to this website.\nTerminate both Remote Desktop sessions.\nResult: You have successfully configured and tested the Azure Firewall. Clean up resources\nRemember to remove any newly created Azure resources that you no longer use. Removing unused resources ensures you will not incur unexpected costs.\nIn the Azure portal, open the Cloud Shell by clicking the first icon in the top right of the Azure Portal. If prompted, click PowerShell and Create storage.\nEnsure PowerShell is selected in the drop-down menu in the upper-left corner of the Cloud Shell pane.\nIn the PowerShell session within the Cloud Shell pane, run the following to remove the resource group you created in this lab:\nRemove-AzResourceGroup -Name \u0026ldquo;AZ500LAB08\u0026rdquo; -Force -AsJob\nClose the Cloud Shell pane.\rConclusion\nIn this lab, I deployed and configured Azure Firewall, gaining hands-on experience in setting up a secure network boundary within an Azure Virtual Network. I created and applied network and application rules to control inbound and outbound traffic, ensuring proper access restrictions based on IP addresses, ports, and applications. Through a remote connection to Srv-work I conducted testing and validation, confirming the effectiveness of the firewall rules. This lab demonstrated Azure Firewall as a robust network security solution, equipping me with practical skills for securing Azure infrastructures. 17\n","date":"1 February 2025","externalUrl":null,"permalink":"/posts/azurefirewall-james-mwangi/","section":"Posts","summary":"A technical walkthrough of Azure Firewall implementation, covering route configuration, DNS management, and firewall rule validation. Demonstrates traffic control between subnets and compliance testing using real-world security scenarios.","title":"Configuring Azure Firewall for Network Security","type":"posts"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/categories/network-administration/","section":"Categories","summary":"","title":"Network Administration","type":"categories"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/network-security/","section":"Tags","summary":"","title":"Network-Security","type":"tags"},{"content":"","date":"1 February 2025","externalUrl":null,"permalink":"/tags/traffic-routing/","section":"Tags","summary":"","title":"Traffic-Routing","type":"tags"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/categories/cybersecurity-strategy/","section":"Categories","summary":"","title":"Cybersecurity Strategy","type":"categories"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/cybersecurity-resilience/","section":"Tags","summary":"","title":"Cybersecurity-Resilience","type":"tags"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/categories/incident-preparedness/","section":"Categories","summary":"","title":"Incident Preparedness","type":"categories"},{"content":"\rMaster the quartely clock. Understand the seasons #\rMangoes are seasonal fruits, and in some parts of the world, nature follows a rhythm of four distinct seasons: Autumn, Summer, Spring, and Winter.\nMastering the quarterly clock and understanding these seasons , is a valuable skill. It allows you to align your activities with predictable cycles, ensuring you’re never caught unprepared. After all, you’ve lived through similar seasons before!\nBelow is a simple image that shows an outline of how you can break down everything you do into 4 parts.\nYou can also break down the year into\nJan- Apr (January,February,March and April) May- Aug (May , June ,July, and August ) Sept - Dec (September , October, November, and December ) This oath to master seasons is echoed through a very popular fable . The African fable tells of a carefree grasshopper 🦗 that neglected to save food during plentiful times. Meanwhile, its neighbors, the ants 🐜, worked to harvest supplies for the drought season. When the dry season arrived, the ants had plenty of food while the grasshopper struggled. What\u0026rsquo;s the lesson from the fable? Prepare for bad seasons during the good ones.\nIn this digital age you need a cybersecurity strategy.\nTherefore ;\n\u0026quot; Align your cybersecurity strategy with quarterly cycles to anticipate risks, optimize preparedness, and thrive in evolving threat landscapes. \u0026quot;\nThe end.\n","date":"30 January 2025","externalUrl":null,"permalink":"/posts/quarterly-clock/","section":"Posts","summary":"Learn how to apply seasonal planning principles—inspired by natural cycles and timeless wisdom—to fortify your cybersecurity operations. By breaking the year into actionable quarters, you can proactively address vulnerabilities, allocate resources strategically, and ensure resilience against both predictable and unforeseen threats.","title":"Master the Quartely Clock","type":"posts"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/categories/operational-security/","section":"Categories","summary":"","title":"Operational Security","type":"categories"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/quarterly-planning/","section":"Tags","summary":"","title":"Quarterly-Planning","type":"tags"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/categories/risk-mitigation/","section":"Categories","summary":"","title":"Risk Mitigation","type":"categories"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/risk-management/","section":"Tags","summary":"","title":"Risk-Management","type":"tags"},{"content":"","date":"30 January 2025","externalUrl":null,"permalink":"/tags/seasonal-cycles/","section":"Tags","summary":"","title":"Seasonal-Cycles","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/ctf/","section":"Tags","summary":"","title":"CTF","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/categories/cyber-forensics/","section":"Categories","summary":"","title":"Cyber Forensics","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/enterprise-networking/","section":"Tags","summary":"","title":"Enterprise-Networking","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/hackthebox/","section":"Tags","summary":"","title":"Hackthebox","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/categories/hackthebox/","section":"Categories","summary":"","title":"HackTheBox","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/ids/","section":"Tags","summary":"","title":"IDS","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/intrusion-detection/","section":"Tags","summary":"","title":"Intrusion-Detection","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/categories/network-configuration/","section":"Categories","summary":"","title":"Network Configuration","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/categories/network-security/","section":"Categories","summary":"","title":"Network Security","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/categories/network-traffic-analysis/","section":"Categories","summary":"","title":"Network Traffic Analysis","type":"categories"},{"content":"\rIntroduction #\rNetwork Traffic Analysis (NTA) involves analyzing network traffic to identify prevalent ports and protocols, establish a baseline for the network environment, monitor and address potential threats, and gain comprehensive insights into the organization\u0026rsquo;s network infrastructure. By enabling security experts to promptly and accurately detect anomalies, such as security threats, NTA plays a crucial role in enhancing network security. Furthermore, NTA supports adherence to security protocols by identifying evolving attack strategies aimed at bypassing detection and leveraging authorized tools within network systems, posing challenges for cybersecurity defenders.\nObjectives: #\r- Enhance understanding of TCP/IP stack \u0026amp; OSI model\n-Analysis using Tcpdump \u0026amp; Wireshark\nKnowledge Check * ?\nNetworking Primer – Layers 1-4 #\rHow many layers does the OSI model have?\n7\nHow many layers are there in the TCP/IP model?\n4\nTrue or False: Routers operate at layer 2 of the OSI model?\nFalse\nWhat addressing mechanism is used at the Link Layer of the TCP/IP model?\nMac-Address\nAt what layer of the OSI model is a PDU encapsulated into a packet? ( the number )\n3\nWhat addressing mechanism utilizes a 32-bit address?\nIPv4\nWhat Transport layer protocol is connection-oriented?\nTCP\nWhat Transport Layer protocol is considered unreliable?\nUDP\nTCP’s three-way handshake consists of 3 packets: 1.Syn, 2.Syn \u0026amp; ACK, 3. _? What is the final packet of the handshake?\nACK\nNetworking Primer — Layers 5–7\nWhat is the default operational mode method used by FTP?\nactive\nFTP utilizes what two ports for command and data transfer? (separate the two numbers with a space)\n20 21\nDoes SMB utilize TCP or UDP as its transport layer protocol?\nTCP\nSMB has moved to using what TCP port?\n445\nHypertext Transfer Protocol uses what well-known TCP port number?\n80\nWhat HTTP method is used to request information and content from the web server?\nGET\nTrue or False: When utilizing HTTPS, all data sent across the session will appear as TLS Application data?\nTrue\nTCPdump Fundamentals #\r(Question-1.zip had an image , attached below)\nUtilizing the output shown in question-1.png, who is the server in this communication? (IP Address)\n174.143.213.184\nWere absolute or relative sequence numbers used during the capture? (see question-1.zip to answer)\nRelative\nIf I wish to start a capture without hostname resolution, verbose output, showing contents in ASCII and hex, and grab the first 100 packets; what are the switches used? please answer in the order the switches are asked for in the question.\n-nvXc 100\nGiven the capture file at /tmp/capture.pcap, what tcpdump command will enable you to read from the capture and show the output contents in Hex and ASCII? (Please use best practices when using switches)\nsudo tcpdump -Xr /tmp/capture.pcap\nWhat TCPDump switch will increase the verbosity of our output? ( Include the — with the proper switch )\n-v\nWhat built-in terminal help reference can tell us more about TCPDump?\nMan\nWhat TCPDump switch will let me write my output to a file?\n-w\nFundamentals Lab\nWhat TCPDump switch will allow us to pipe the contents of a pcap file out to another function such as ‘grep’?\n-l\nTrue or False: The filter “port” looks at source and destination traffic.\nTrue\nIf we wished to filter out ICMP traffic from our capture, what filter could we use? ( word only, not symbol please.)\nnot icmp\nWhat command will show you where / if TCPDump is installed?\nwhich tcpdump\nHow do you start a capture with TCPDump to capture on eth0?\ntcpdump -i eth0\nWhat switch will provide more verbosity in your output?\n-v\nWhat switch will write your capture output to a .pcap file?\n-w\nWhat switch will read a capture from a .pcap file?\n-r\nWhat switch will show the contents of a capture in Hex and ASCII?\n-X\nTcpdump Packet Filtering #\rWhat filter will allow me to see traffic coming from or destined to the host with an ip of 10.10.20.1?\nhost 10.10.20.1\nWhat filter will allow me to capture based on either of two options?\nor\nTrue or False: TCPDump will resolve IPs to hostnames by default\nTrue\nInterrogating Network Traffic With Capture and Display Filters #\r(The section requires we unzip TCPDump-lab-2.zip)\nWhat are the client and server port numbers used in first full TCP three-way handshake? (low number first then high number)\n80 43806\nBased on the traffic seen in the pcap file, who is the DNS server in this network segment? (ip address)\n172.16.146.1\nAnalysis with Wireshark\nTrue or False: Wireshark can run on both Windows and Linux.\nTrue\nWhich Pane allows a user to see a summary of each packet grabbed during the capture?\nPacket List\nWhich pane provides you insight into the traffic you captured and displays it in both ASCII and Hex?\nPacket Bytes\nWhat switch is used with TShark to list possible interfaces to capture on?\n-D\nWhat switch allows us to apply filters in TShark?\n-f\nIs a capture filter applied before the capture starts or after? (answer before or after)\nBefore\nWireshark Advanced Usage #\rWhich plugin tab can provide us with a way to view conversation metadata and even protocol breakdowns for the entire PCAP file?\nStatistics\nWhat plugin tab will allow me to accomplish tasks such as applying filters, following streams, and viewing expert info?\nAnalyze\nWhat stream-oriented Transport protocol enables us to follow and rebuild conversations and the included data?\nTCP\nTrue or False: Wireshark can extract files from HTTP traffic.\nTrue\nTrue or False: The ftp-data filter will show us any data sent over TCP port 21.\nFalse\nPacket Inception, Dissecting Network Traffic With Wireshark #\rWhat was the filename of the image that contained a certain Transformer Leader? (name.filetype)\nRise-Up.jpg\nWhich employee is suspected of performing potentially malicious actions in the live environment?\nBob\nGuided Lab: Traffic Analysis Workflow #\rWhat was the name of the new user created on Mr. Ben’s host?\nhacker\nHow many total packets were there in the Guided-analysis PCAP?\n44\nWhat was the suspicious port that was being used?\n4444\nDecrypting RDP connections #\rWhat user account was used to initiate the RDP connection?\nBucky\nGlad to share this achievement: https://academy.hackthebox.com/achievement/1317759/81\nConclusion #\rCompleting the Network Traffic Analysis module has given me a solid foundation in analyzing and interpreting network data, which is essential in cybersecurity. By working hands-on with tools like Wireshark and tcpdump, I’ve learned to spot malicious patterns and leverage intrusion detection systems effectively. This experience has strengthened my skills in detecting network anomalies and has equipped me with practical tools and methodologies that I can apply in real-world network defense and threat intelligence scenarios.\n","date":"23 October 2024","externalUrl":null,"permalink":"/posts/network-traffic-analysis/","section":"Posts","summary":"Networking article series","title":"Network Traffic Analysis","type":"posts"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/network-configuration/","section":"Tags","summary":"","title":"Network-Configuration","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/packet-inspection/","section":"Tags","summary":"","title":"Packet-Inspection","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/packet-tracer/","section":"Tags","summary":"","title":"Packet-Tracer","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/radius/","section":"Tags","summary":"","title":"RADIUS","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/tcpdump/","section":"Tags","summary":"","title":"Tcpdump","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/traffic-analysis/","section":"Tags","summary":"","title":"Traffic-Analysis","type":"tags"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/categories/wireless-networks/","section":"Categories","summary":"","title":"Wireless Networks","type":"categories"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/wireless-security/","section":"Tags","summary":"","title":"Wireless-Security","type":"tags"},{"content":"\rIntroduction #\rA Wireless Local Area Network (WLAN) leverages radio frequency transmissions to establish network connectivity amongst co-located devices within a defined geographical area. This eliminates the need for physical cabling, facilitating user mobility while maintaining network access.\nObjectives: #\rIn this activity, I will configure both a wireless home router and a Wireless local controller (WLC)-based network. I will implement both WPA2-PSK and WPA2-Enterprise security. Configure a home router to provide Wi-Fi connectivity to a variety of devices. Configure WPA2-PSK security on a home router. Configure interfaces on a WLC. Configure WLANs on a WLC. Configure WPA2-PSK security on a WLAN and connect hosts to a WLAN. Configure WPA2-Enteprise on a WLAN and connect hosts to the WLAN. Verify connectivity \u0026amp; WLAN connectivity\nPart 1: Configure a Home Wireless Router #\rTo change the DHCP settings I access the Home Wireless Router GUI and change the router IP and DHCP settings according to the information in the Addressing Table. I set the maximum number of users to 20. Start IP address from 3. The static DNS server with the address 10.100.100.252. The internet IP address 10.100.200.2 /27. The subnet /27 is 255.255.255.0\nTo configure WLAN, I used the 2.4GHz WLAN interface in the Basic Wireless Settings tab. The SSID name is Home SSID. I choose the standard channel 6 – 2.437 GHz. I enabled the SSID broadcast\nTo configure security in the wireless security tab I choose the WPA2 Personal option and the passphrase as Cisco123.\nI also changed the default password to Cisco123. Default passwords are common and easy to be compromised thus the need to change it.\nNext, I connect clients to the network. I opened the PC Wireless app on the desktop of the laptop. I picked the SSID named Home SSID which I configured earlier with the password Cisco123.\nThe laptop once connected had some parallel lines connecting it to the router representing the wireless connection was active\nFor the smartphone I configured it under the config wireless panel to the same SSID and credentials as the tablet and the laptop.\nThe tablet configuration is shown above.\nTo verify the connection was active I performed ping from the laptop to 192.168.6.5- Smartphone, 192.168.6.5-tablet, to the webserver IP address 203.0.113.78 and www.netacad.pt . The ping were successful.\nPart 2: Configure a WLC Controller Network #\rIn this part one WLAN will use WPA2-PSK authentication. The other WLAN will use WPA2-Enterprise authentication. I will also configure the WLC to use an SNMP server and configure a DHCP scope that will be used by the wireless management network.\nTo configure VLAN interfaces I accessed the Enterprise Admin and navigated to WLC-1 management interface via a web browser. To log into WLC-1,I used admin as the username and Cisco123 as the password.\nThe first WLAN was WLAN 2 controller The setting for WLAN 2 can be seen below\nTo configure WLAN 5, I followed similar steps as WLAN 2\nThe next step is to configure a DHCP scope for the wireless management network with the specification offered in the lab.\nIn step 3, I configured the WLC with external server addresses. Under the Security tab I configured a new RADIUS authentication server with index 1.\nI also configured a WLC to send logs information to an SNMP server. Community Name: WLAN\nIP address: 10.6.0.254\nIn step 4, I created the WLAN’s and enabled them\nThe WPA2-PSK security was used by Wireless VLAN 2. Also had to enable Enable FlexConnect Local Switching and FlexConnect Local Auth.\nThe second profile Wireless VLAN 5 used 802.1x - WPA2-Enterprise security . I configured it accordingly.\nIn step 5 , Wireless Host 1 should connect to Wireless VLAN 2.Wireless Host 2 should connect to Wireless VLAN 5 using the credentials in the WLAN information table.\nFrom Wireless Host 1 I selected SSID 2 and typed the password as Cisco123. The connection went through.\nSince Wireless VLAN 5 used 802.1x - WPA2-Enterprise security, the connection had to happen via a profile creation using the details provided in the addressing table.\nBoth Wireless hosts were now connected to the network.\nTo test the network I ping the IP and URL to the webserver www.netacad.pt .\nWireless host 1:\nWireless host 2:\nConclusion #\rThrough this comprehensive laboratory experience with Packet Tracer, I gained practical expertise in implementing both residential and enterprise wireless security solutions. The progression from configuring a basic wireless home router to implementing a WLC-based network provided invaluable hands-on experience with both WPA2-PSK and WPA2-Enterprise security protocols. By methodically working through each configuration step - from setting up wireless interfaces and establishing WLANs on the WLC to implementing varying levels of security protocols - I developed a thorough understanding of secure wireless deployment practices. The practical verification of connectivity across different security configurations demonstrated the real-world implications of each implementation choice, reinforcing the importance of proper security protocol selection and configuration in wireless networking.\n_\n","date":"23 October 2024","externalUrl":null,"permalink":"/posts/wlan-configuration/","section":"Posts","summary":"Networking configuration article series","title":"WLAN Configuration","type":"posts"},{"content":"","date":"23 October 2024","externalUrl":null,"permalink":"/tags/wlan-security/","section":"Tags","summary":"","title":"Wlan-Security","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/tags/aws-security/","section":"Tags","summary":"","title":"Aws Security","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/categories/aws-security/","section":"Categories","summary":"","title":"Aws Security","type":"categories"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/tags/cloud-vulnerabilities/","section":"Tags","summary":"","title":"Cloud Vulnerabilities","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/tags/encryption/","section":"Tags","summary":"","title":"Encryption","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/tags/flaws.cloud/","section":"Tags","summary":"","title":"Flaws.cloud","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/categories/flaws.cloud/","section":"Categories","summary":"","title":"Flaws.cloud","type":"categories"},{"content":"\rIntroduction #\rIn the Flaws AWS (Amazon Web Service) series of levels, we will learn about common mistakes and pitfalls when using Amazon Web Services (AWS). By participating in this challenge, we will gain hands-on experience with AWS-specific issues and learn how to avoid them in our own projects. As we progress through the levels, we will confront various security vulnerabilities that commonly occur in AWS environments, where our task is to identify and exploit these vulnerabilities, gaining insights into how they can be mitigated. The goal is to provide us with a deeper understanding of AWS security best practices. Each level presents a scenario with a specific AWS security flaw, and we will learn how to discover and exploit these vulnerabilities in a controlled environment. A series of hints will guide us through each level, teaching us the commands and techniques needed to uncover the information required to solve the challenges.\nObjective: #\rS3 Bucket Permissions: Understand and correct overly permissive S3 bucket policies. Identify an S3 bucket that is publicly accessible and contains sensitive information. Learn how to use AWS Identity and Access Management (IAM) policies to restrict access to S3 buckets. AWS CLI Access : Use the AWS Command Line Interface (CLI) to retrieve and find sensitive data. Understand the importance of being aware of the scope of permissions granted, especially when using broad permissions like Any Authenticated AWS User preventing unauthorized data retrieval.\nIAM access keys: Identify an IAM role with excessive permissions and exploit it to access other AWS s3 bucket resources. See the importance of always revoking any AWS keys or any secrets that could have been leaked or were misplaced.\nProtect Backups and Snapshots:Implement stringent access controls to secure backups and snapshots of EC2 instances and databases, as they can be exploited by attackers to gain unauthorized access.\nSecure Metadata Service Access: Ensure the metadata service endpoint (169.254.169.254) is properly secured to prevent the exposure of sensitive information\nMonitor Read-Only Permissions :Identify why it’s proactive to be cautious with read-only permissions, such as the SecurityAudit policy, as they can inadvertently aid attackers in identifying vulnerabilities. Regularly review and manage these permissions to maintain a secure environment.\nLevel 1 : #\rThe site flaws.cloud is hosted as an S3 bucket. This is a great way to host a static site, similar to hosting one via github pages. to confirm the hosting . You can determine the site is hosted as an S3 bucket by running a DNS lookup on the domain, such as:\n$dig +nocmd flaws.cloud any +multiline +noall +answer\nTo know the region of the s3 bucket we do an nslookup on flaws.cloud and sub sequent nslookup on one of the returned IP address as shown below\nfrom the displayed information the s3 bucket is located in the “us-west-2” region. If we browse the bucket via aws s3 ls s3://flaws.cloud/ \u0026ndash;no-sign-request \u0026ndash;region us-west-2\nwe can see there is a secret html page\nFinally, we can also just visit http://flaws.cloud.s3.amazonaws.com/ which lists the files due to the permissions issues on this bucket.\nfrom here we can get the secret and access it from : http://flaws.cloud.s3.amazonaws.com/secret-dd02c7c.html\nThe page takes us to level 2. Lesson learned level 1\nOn AWS you can set up S3 buckets with all sorts of permissions and functionality including using them to host static files. A number of people accidentally open them up with permissions that are too loose. Just like how you shouldn\u0026rsquo;t allow directory listings of web servers, you shouldn\u0026rsquo;t allow bucket listings.\nif proprietary data is uploaded to the bucket then it would be exposed to attackers and everyone on the internet. Level 2: #\rSimilar to the first level, you can discover that this sub-domain is hosted as an S3 bucket with the name \u0026ldquo;level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud\u0026rdquo;. Its permissions are too loose, but you need your own AWS account to see what\u0026rsquo;s inside. Using my account I run:\naws s3 \u0026ndash;profile hourglass ls s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud\nfrom the terminal results we can access the flag page via\nhttp://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud/secret-e4443fc.html\nalternatively we can download all the files via sync and open the secret html page in a browser : file:///home/cyberchaosjedi/Music/Friday/secret-e4443fc.html\nLesson learned level2\nSimilar to opening permissions to \u0026ldquo;Everyone\u0026rdquo;, people accidentally open permissions to \u0026ldquo;Any Authenticated AWS User\u0026rdquo;. They might mistakenly think this will only be users of their account, when in fact it means anyone that has an AWS account.\nLevel 3: #\rWe download the level 3 files via sync :\naws s3 sync s3://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud/ . \u0026ndash;no-sign-request \u0026ndash;region us-west-2\nto view all files including the hidden ones we can use : ls -la\nfrom the list we have a .git file. People often accidentally add secret things to git repos, and then try to remove them without revoking or rolling the secrets. You can look through the history of a git repo by running:\ngit log\nThere are 2 versions of the commit. Then you can look at what a git repo looked like at the time of a commit by running:\ngit checkout f7cebc46b471ca9838a0bdd1074bb498a3f84c87\nwhere `f7cebc46b471ca9838a0bdd1074bb498a3f84c87` would be the hash for the commit shown in `git log`.\ngit checkout f52ec03b227ea6094b04e43f475fb0126edb5a61 if we list we can find a sensitive key file that was left\ncat access_key.txt\nwe can configure a profile flawsaws with the credentials , following is the “who am I “ identity check using aws sts get-caller-identity – profile flawaws\nThen to list S3 buckets using that profile run:\naws \u0026ndash;profile flawsaws s3 ls\nWe have listed all the buckets available in the profile. the next level 4 : http://level4-1156739cfb264ced6de514971a4bef68.flaws.cloud/\nLesson learned level 3\nPeople often leak AWS keys and then try to cover up their mistakes without revoking the keys. You should always revoke any AWS keys (or any secrets) that could have been leaked or were misplaced. Roll your secrets early and often.\nAnother interesting issue this level has exhibited, although not that worrisome, is that you can\u0026rsquo;t restrict the ability to list only certain buckets in AWS, so if you want to give an employee the ability to list some buckets in an account, they will be able to list them all. The key you used to discover this bucket can see all the buckets in the account. You can\u0026rsquo;t see what is in the buckets, but you\u0026rsquo;ll know they exist. Similarly, be aware that buckets use a global namespace meaning that bucket names must be unique across all customers, so if you create a bucket named `merger_with_company_Y` or something that is supposed to be secret, it\u0026rsquo;s technically possible for someone to discover that bucket exists.\nLevel 4: #\rYou can snapshot the disk volume of an EC2 as a backup. In this case, the snapshot was made public, but you\u0026rsquo;ll need to find it.\nTo do this, first we need the account ID, which we can get using the AWS key from the previous level:\naws \u0026ndash;profile flaws sts get-caller-identity\nthe account is 975426262029\nThe ARN shows the account is /backup . The backups this account makes are snapshots of EC2s. Next, discover the snapshot: aws \u0026ndash;profile flawsaws ec2 describe-snapshots \u0026ndash;owner-id 975426262029\nBy default snapshots are private, and you can transfer them between accounts securely by specifying the account ID of the other account, but a number of people just make them public and forget about them it seems.\nNow that you know the snapshot ID, you\u0026rsquo;re going to want to mount it. You\u0026rsquo;ll need to do this in your own AWS account.\nFirst, create a volume using the snapshot:\naws \u0026ndash;profile hourglass ec2 create-volume \u0026ndash;availability-zone us-west-2a \u0026ndash;region us-west-2 \u0026ndash;snapshot-id snap-0b49342abd1bdcb89\nchoose the volume you just created. create a new ec2 instance , an rsa pair key and SSH into the instance\nto get the ssh link , click the instance and select Connect\nGrant read access chmod 400\nthen ssh -i \u0026ldquo;flaws-access-key.pem\u0026rdquo; ec2-user@ec2-54-190-59-246.us-west-2.compute.amazonaws.com\nWe\u0026rsquo;ll need to mount this extra volume by running:\nlsblk\nsudo file -s /dev/xvdb1\n# Next we mount it sudo mount /dev/xvdb1 /mnt followed by verification\nwe navigate to /mnt , home folder , ubuntu and check the content of the nginx configuration file\nhere we get credentials to log into : http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/\nLesson learned level 4\nAWS allows you to make snapshots of EC2\u0026rsquo;s and databases (RDS). The main purpose for that is to make backups, but people sometimes use snapshots to get access back to their own EC2\u0026rsquo;s when they forget the passwords. This also allows attackers to get access to things. Snapshots are normally restricted to your own account, so a possible attack would be an attacker getting access to an AWS key that allows them to start/stop and do other things with EC2\u0026rsquo;s and then uses that to snapshot an EC2 and spin up an EC2 with that volume in your environment to get access to it. Like all backups, you need to be cautious about protecting them.\nLevel 5: #\rThis EC2 has a simple HTTP only proxy on it. Here are some examples of it\u0026rsquo;s usage:\nhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/flaws.cloud/\nhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/summitroute.com/blog/feed.xml\nhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/neverssl.com/\nHTTP may be vulnerable to SSRF (Server-Side Request Forgery) . We can try accessing the metadata\nvia http://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/meta-data\n169.254.169.254/latest/meta-data is a well known way of accessing the meta data of a service.\nThen :\nhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/meta-data/iam\nhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/meta-data/iam/security-credentials/\nhttp://4d0cf09b9b2d761a7d87be99d17507bce8b86f3b.flaws.cloud/proxy/169.254.169.254/latest/meta-data/iam/security-credentials/flaws\nWe have obtained access keys credentials used to run the IAM access , we can create a profile with cli access\nthen nano ~/.aws/credentials and add the session token found with the IAM role credentials.\nWe are to figure out how to list the contents of the level6 bucket at level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud that has a hidden directory in it.\nUsing the created profile we list the contents\nthe directory is ddcc78ff and with a index.html\nhttp://level6-cc4c404a8a8b876167f5e70a7d8c9880.flaws.cloud/ddcc78ff/index.html\nLesson learned level 5\nThe IP address 169.254.169.254 is a magic IP in the cloud world. AWS, Azure, Google, DigitalOcean and others use this to allow cloud resources to find out metadata about themselves. Some, such as Google, have additional constraints on the requests, such as requiring it to use `Metadata-Flavor: Google` as an HTTP header and refusing requests with an `X-Forwarded-For` header. AWS has recently created a new IMDSv2 that requires special headers, a challenge and response, and other protections, but many AWS accounts may not have enforced it. If you can make any sort of HTTP request from an EC2 to that IP, you\u0026rsquo;ll likely get back information the owner would prefer you not see.\nA similar problem to getting access to the IAM profile\u0026rsquo;s access keys is access to the EC2\u0026rsquo;s user-data, which people sometimes use to pass secrets to the EC2 such as API keys or credentials.\nLevel 6: #\rWe set up a profile named flaws6 . using the provided IAM access key and secret key\nThe SecurityAudit group can get a high-level overview of the resources in an AWS account, but it\u0026rsquo;s also useful for looking at IAM policies. First, find out who you are (assuming you named your profile \u0026ldquo;level6\u0026rdquo;): aws \u0026ndash;profile flaws6 iam get-user\nWe find out the user is named level6\nTo find out what policies are attached to it:\naws \u0026ndash;profile flaws6 iam list-attached-user-policies \u0026ndash;user-name Level6\nthere are two policies attached\n\u0026ldquo;SecurityAudit\u0026rdquo;: This is an AWS managed policy that provides permissions necessary for security assessments. It is designed to allow users or roles to view (but not modify) a wide range of security-relevant resources and settings in an AWS account. \u0026ldquo;list_apigateways\u0026rdquo; a custom made policy. Once you know the ARN for the policy you can get it\u0026rsquo;s version id:\naws \u0026ndash;profile flaws6 iam get-policy \u0026ndash;policy-arn arn:aws:iam::975426262029:policy/list_apigateways\nThe policy id : \u0026ldquo;PolicyId\u0026rdquo;: \u0026ldquo;ANPAIRLWTQMGKCSPGTAIO\u0026rdquo;,\nNow that we have the ARN and the version id, we can see what the actual policy is:\naws \u0026ndash;profile flaws6 iam get-policy-version \u0026ndash;policy-arn arn:aws:iam::975426262029:policy/list_apigateways \u0026ndash;version-id v4\nThis tells us using this policy we can call \u0026ldquo;apigateway:GET\u0026rdquo; on \u0026ldquo;arn:aws:apigateway:us-west-2::/restapis/*\u0026rdquo;\nThe API gateway in this case is used to call a lambda function, but we need to figure out how to invoke it.\nThe SecurityAudit policy lets us see some things about lambdas:\naws \u0026ndash;region us-west-2 \u0026ndash;profile flaws6 lambda list-functions\nThat tells you there is a function named \u0026ldquo;Level6\u0026rdquo;, and the SecurityAudit also lets you run:\naws \u0026ndash;region us-west-2 \u0026ndash;profile flaws6 lambda get-policy \u0026ndash;function-name Level6\nThis tells us about the ability to execute `arn:aws:execute-api:us-west-2:975426262029:s33ppypa75/*/GET/level6\\` That \u0026ldquo;s33ppypa75\u0026rdquo; is a rest-api-id, which we can then use with the other attached policy:\naws \u0026ndash;profile flaws6 \u0026ndash;region us-west-2 apigateway get-stages \u0026ndash;rest-api-id \u0026ldquo;s33ppypa75\u0026rdquo;\nThat tells us the stage name is \u0026ldquo;Prod\u0026rdquo;. Lambda functions are called using the rest-api-id, stage name, region, and resource as https://s33ppypa75.execute-api.us-west-2.amazonaws.com/Prod/level6\n\u0026ldquo;Go to http://theend-797237e8ada164bf9f12cebf93b282cf.flaws.cloud/d730aa2b/\u0026rdquo;\nthe link navigates to the page :\nLesson learned level 6\nIt is common to give people and entities read-only permissions such as the SecurityAudit policy. The ability to read your own and other\u0026rsquo;s IAM policies can really help an attacker figure out what exists in your environment and look for weaknesses and mistakes.\nConclusion #\rThe flAWS.cloud challenge expounds the importance of rigorous security management in AWS environments. There is the necessity of carefully configuring permissions to avoid accidental data exposure, such as ensuring S3 buckets are not overly permissive. Understanding permission scopes is vital to prevent unauthorized access, especially when granting broad permissions that might be misconstrued, such as \u0026ldquo;Any Authenticated AWS User.\u0026rdquo; Promptly revoking and rotating compromised AWS keys is crucial to maintaining security. Additionally, safeguarding backups and snapshots is essential, as these can be exploited by attackers to gain unauthorized access. Properly securing the metadata service endpoint (169.254.169.254) is also critical, as it can expose sensitive information if not protected by measures like IMDSv2. Finally, even read-only permissions like the SecurityAudit policy can help attackers identify vulnerabilities if not managed carefully. Overall, the challenge highlights the need for diligent permission management, proactive credential handling, and vigilant protection of sensitive data to ensure a secure AWS environment.\n","date":"16 September 2024","externalUrl":null,"permalink":"/posts/flawsaws/","section":"Posts","summary":"Cloud security strategy article series","title":"Flaws.cloud Common AWS Vulnerabilities","type":"posts"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/tags/key-management/","section":"Tags","summary":"","title":"Key Management","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/tags/security-best-practices/","section":"Tags","summary":"","title":"Security Best Practices","type":"tags"},{"content":"","date":"16 September 2024","externalUrl":null,"permalink":"/categories/vulnerabilities/","section":"Categories","summary":"","title":"Vulnerabilities","type":"categories"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/tags/aws/","section":"Tags","summary":"","title":"Aws","type":"tags"},{"content":"\rIntroduction #\rThe AWS Key Management Service (KMS) provides secure storage and rotation of encryption keys with robust access control. For a cloud security engineer, it\u0026rsquo;s crucial to have a comprehensive understanding of AWS KMS to ensure the security and integrity of data. Mastery of AWS KMS involves learning how to create, manage, and use cryptographic keys effectively within AWS services and applications. This entails understanding key policies, utilizing both customer-managed and AWS-managed keys, enabling automatic key rotation, and integrating KMS with other AWS services to ensure consistent enforcement of data encryption and access controls.\nObjective: #\rDemonstrate the integration of AWS Key Management Service (KMS) with AWS services, such as Amazon S3 and Amazon EBS, to streamline the encryption and decryption process. This involves enabling data encryption within these services without requiring manual key management, showcasing how KMS simplifies secure data handling in cloud environments.\nLab implementation #\rLogin to your AWS Web console (root account) and create an IAM account.\n#\rI created an IAM user “kenya-one” and assigned the user admin privileges. Creating an IAM (Identity and Access Management) user account is recommended rather than using the root user account. An IAM user account provides a more secure way to access and manage AWS resources and services. By creating an IAM user account, you can specify the user’s permissions and restrict their access to only the necessary AWS resources and services. All the activities will be done in the IAM account.\nClick on Services. In the Search field, type KMS and then select Key Management Service. On the KMS Console page, click on Create Key.\nThere are two types of encryption, asymmetric and symmetric\nSymmetric encryption, involves using a single key for both encryption and decryption of data. This key must be securely shared between the communicating parties. Symmetric encryption is faster and more computationally efficient compared to asymmetric encryption, making it suitable for bulk data encryption. Used for encrypting data at rest (eg, EBS volumes, S3 objects) and protecting data in transit (eg, between AWS services).\nAsymmetric encryption, on the other hand, uses a pair of keys: a public key and a private key. The public key is used for encryption, while the private key is used for decryption. It enables secure communication and authentication without the need to share private keys, thus enhancing security in distributed systems and applications. Used for Secure key exchange (eg, encrypting symmetric keys for secure transmission), digital signatures for data integrity and authentication, and SSL/TLS encryption for secure web traffic.\nThere are five steps to the creation of the KMS: In the Configure key (Step 1), retain the default settings. Click on Next to continue.\nUnder Add Label (Step 2): Enter an alias of your choice as the name of your master key.\nThe description is optional. Click Next.\nDefine key administrative permissions (Step 3)\nTo allow users to perform encryption and decryption, I have to assign key permissions to them on the Define key administrative permissions page. Permit the user created in the previous lab to use the kenya-one-key by selecting the checkbox near the user. Here, we have selected the user kenya-one we created in the previous lab. Click on Next to continue.\nDefine key usage permissions (Step 4) . Here, you are trying to give the user key permission. You have the option to add another AWS user account. Select the user kenya-one and click on Next to continue.\nReview (Step 5) Review all settings and the key policy, which is in JSON format, and then click FINISH to create the master key. Now we have successfully created an AWS KMS for the IAM user kenya-one\nIntegrating the Created KMS to AWS services such as Amazon S3 and Amazon EBS\nLet\u0026rsquo;s start with Amazon S3 . In your AWS Management Console, type S3 in the search field to create a new bucket. On your S3 bucket page. Click an S3 Create bucket. The Create bucket popup appears. Under General Configuration, type the name of the bucket in the Bucket Name field -here, the bucket name is kenya-one-bucket.\nRetain the default Object Ownership setting and block this bucket\u0026rsquo;s public access settings. #\rEnable bucket versioning and retain the default tags and default encryption settings. Then, click on Create Bucket. The ‘kenya-one-bucket’ is created successfully.\nClick on the newly created bucket to configure the encryption settings Click on the bucket name ‘kenya-one-bucket’ to navigate the properties of the bucket. In the properties tab, ensure bucket versioning is enabled. Next, under default encryption, we select Server-side encryption with AWS Key Management Service keys (SSE-KMS).\nRetain the default setting (Enable) for Bucket Key and click on Save Changes.\nSuccess notification\nScroll down and click on Edit under Server Access Logging. Enable Server access logging by selecting Enable, entering s3:// kenya-one-bucket or you can click on “Browse S3” to add the Target bucket, and click on Save changes.\nSuccess notification. We have configured the ‘kenya-one-bucket’ to encrypt data. The user can now add data\nAlso, the ARN chosen in the Default Encryption section is “kenya-one-key”.\nKMS to Amazon EBS #\rAmazon EBS supports KMS. Its encryption provides data-at-rest security by encrypting data volumes, boot volumes, and snapshots using Amazon-managed keys or keys created and managed using AWS KMS.\nTo encrypt EBS volumes with the KMS master key, click on Services from the menu bar and search for EC2. From the search results, click on EC2 virtual servers in the cloud.\nOnce the EC2 Dashboard page opens, click on Volumes in the left pane under ELASTIC BLOCK STORE and click on Create Volume.\nIn the KMS Key field, select the kenya-one-key that was created: and click Create Volume\nMini Task\nNow, encrypt Amazon Redshift using the same KMS master key as you encrypted the EBS volume. By following the above steps, you can now as a cloud security engineer implement AWS KMS. Search for redshift in the services.\nCreate a workgroup (here, I named it ‘kenya-one-wg’)\nClick next and create a namespace(here , I named it ‘kenya-one-ns’)\nUnder the namespace creation page, head over to encryption and select add encryption and security. Paste in the ARN for our kenya-one-key created earlier .\nClick next, Review, and create.\nConclusion #\rImplementing AWS Key Management Service (KMS) enhances data security in the cloud by providing comprehensive key management capabilities, including creation, rotation, and access control, integrated seamlessly with other AWS services. It simplifies compliance with regulatory requirements through detailed logging and auditing, supports both symmetric and asymmetric keys for diverse use cases, and scales accordingly to meet growing organisational needs. By adopting AWS KMS, organizations can effectively protect sensitive data, streamline key management processes, reduce operational overhead, and ensure adherence to industry standards and best practices.\n","date":"20 August 2024","externalUrl":null,"permalink":"/posts/aws-key/","section":"Posts","summary":"Cloud security strategy article series","title":"AWS Key Management Implementation","type":"posts"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/tags/cloud/","section":"Tags","summary":"","title":"Cloud","type":"tags"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/categories/cloud/","section":"Categories","summary":"","title":"Cloud","type":"categories"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/categories/encryption/","section":"Categories","summary":"","title":"Encryption","type":"categories"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/tags/keys/","section":"Tags","summary":"","title":"Keys","type":"tags"},{"content":"","date":"20 August 2024","externalUrl":null,"permalink":"/tags/security/","section":"Tags","summary":"","title":"Security","type":"tags"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/tags/flsm/","section":"Tags","summary":"","title":"FLSM","type":"tags"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/categories/ip-management/","section":"Categories","summary":"","title":"IP Management","type":"categories"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/tags/ip-addressing/","section":"Tags","summary":"","title":"IP-Addressing","type":"tags"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/categories/network-design/","section":"Categories","summary":"","title":"Network Design","type":"categories"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/tags/network-management/","section":"Tags","summary":"","title":"Network-Management","type":"tags"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/tags/subnetting/","section":"Tags","summary":"","title":"Subnetting","type":"tags"},{"content":"\rUnderstanding IP Subnetting: A Comprehensive Guide #\rIntroduction #\rIn today\u0026rsquo;s interconnected digital landscape, efficient network management is crucial for organizations of all sizes. IP subnetting stands as one of the fundamental techniques that network administrators employ to organize, optimize, and secure their network infrastructure. Subnetting is the practice of dividing a larger IP network into smaller, more manageable subnetworks or \u0026ldquo;subnets.\u0026rdquo; This logical subdivision enables better resource allocation, enhanced security, and improved network performance.\nAs defined by networking professionals, a subnet is a logical subdivision of an IP network that operates within its unique range of IP addresses. Think of it as partitioning a large office space into smaller departments, each with its distinct area and resources. This article delves into the intricacies of subnetting, exploring both Variable Length Subnet Mask (VLSM) and Fixed Length Subnet Mask (FLSM) approaches, with practical examples to illustrate these concepts.\nThe Fundamentals of Subnetting #\rWhy Subnet? #\rBefore diving into the technical aspects, it\u0026rsquo;s important to understand why subnetting is necessary:\nEfficient Address Utilization: Subnetting allows for more efficient use of limited IP address space, particularly important with IPv4\u0026rsquo;s constraints.\nNetwork Management: Breaking down a large network into smaller subnets makes management, troubleshooting, and maintenance significantly easier.\nReduced Network Congestion: Subnets create separate broadcast domains, limiting broadcast traffic and improving overall network performance.\nEnhanced Security: Subnetting facilitates the implementation of security measures between different network segments, controlling traffic flow between subnets.\nGeographical Organization: Organizations can align their network structure with physical locations or departments.\nUnderstanding IP Addresses and Subnet Masks #\rAn IPv4 address consists of 32 bits, typically represented in decimal format with four octets separated by periods (e.g., 192.168.1.1). The subnet mask determines which portion of the IP address represents the network and which portion identifies the host.\nFor example, in a subnet with the mask 255.255.255.0 (or /24 in CIDR notation), the first 24 bits identify the network, while the last 8 bits identify individual hosts within that network.\nFixed Length Subnet Mask (FLSM) #\rFLSM represents the traditional approach to subnetting, where all subnets within a network have the same subnet mask. This means each subnet has an identical number of available host addresses, regardless of actual needs.\nFLSM in Practice #\rLet\u0026rsquo;s examine several examples to understand the practical application of FLSM:\nExample 1: 192.168.10.0/25 #\rNumber of possible subnets: 2^(32-25) = 2^7 = 128 Number of usable hosts per subnet: 2^(32-25) - 2 = 126 Network addresses: 192.168.10.0/25, 192.168.10.128/25 First usable IP address: 192.168.10.1 Last usable IP address: 192.168.10.126 Broadcast address: 192.168.10.127 In this example, we\u0026rsquo;re using a /25 subnet mask on the 192.168.10.0 network. This creates two subnets, each with 126 usable host addresses. The first subnet ranges from 192.168.10.1 to 192.168.10.126, while the second ranges from 192.168.10.129 to 192.168.10.254.\nExample 2: 10.0.0.0/16 #\rNumber of possible subnets: 2^(32-16) = 2^16 = 65,536 Number of usable hosts per subnet: 2^(32-16) - 2 = 65,534 Network addresses: 10.0.0.0/16, 10.1.0.0/16, 10.2.0.0/16, etc. First usable IP address: 10.0.0.1 Last usable IP address: 10.0.255.254 Broadcast address: 10.0.255.255 This example demonstrates a much larger subnet with a /16 mask, providing 65,534 usable host addresses within a single subnet. This would be suitable for very large network segments.\nLimitations of FLSM #\rWhile FLSM is straightforward to implement, it has notable limitations:\nInefficient Address Allocation: All subnets have the same size, leading to wasted addresses in smaller departments or segments. Inflexibility: Cannot adapt to varying needs across different parts of the network. Wasteful for Point-to-Point Links: Point-to-point connections only need two addresses but might receive many more in an FLSM scheme. These limitations led to the development of a more flexible approach: Variable Length Subnet Mask (VLSM).\nVariable Length Subnet Mask (VLSM) #\rVLSM represents an evolution in subnetting technology, allowing network administrators to use different subnet masks for different subnets within the same network. This approach enables tailoring subnet sizes to actual requirements, significantly improving IP address utilization.\nVLSM in Practice #\rLet\u0026rsquo;s examine a real-world VLSM implementation using the 192.168.5.0 network:\nNetwork Network ID IP Range Hosts Per Subnet Broadcast ID B 192.168.5.0/27 192.168.5.1 - 192.168.5.30 30 192.168.5.31 E 192.168.5.32/27 192.168.5.33 - 192.168.5.62 30 192.168.5.63 A 192.168.5.64/28 192.168.5.65 - 192.168.5.78 14 192.168.5.79 D 192.168.5.80/28 192.168.5.81 - 192.168.5.94 14 192.168.5.95 C 192.168.5.96/30 192.168.5.97 - 192.168.5.98 2 192.168.5.99 In this VLSM implementation:\nNetworks B and E use a /27 mask, providing 30 usable host addresses each Networks A and D use a /28 mask, providing 14 usable host addresses each Network C uses a /30 mask, providing just 2 usable addresses, ideal for a point-to-point link Benefits of VLSM #\rVLSM offers several advantages over FLSM:\nEfficient Address Utilization: Allocates only the necessary number of addresses to each subnet, minimizing waste.\nFlexibility: Accommodates networks of varying sizes within the same overall network space.\nScalability: Allows for easier network growth and reconfiguration as needs evolve.\nOptimized for Special Cases: Perfect for point-to-point links that only require two IP addresses.\nVLSM Implementation Considerations #\rWhile VLSM offers significant benefits, it also introduces complexity:\nCareful Planning Required: Subnet boundaries must be properly calculated to avoid overlap. Detailed Documentation: More complex subnet schemes require thorough documentation. Advanced Routing Protocols: VLSM typically requires classless routing protocols like OSPF or EIGRP. Higher Technical Expertise: Network administrators need deeper understanding of binary mathematics and IP addressing. Practical Subnetting Techniques #\rSubnetting Steps #\rTo effectively subnet a network, follow these key steps:\nIdentify Requirements: Determine how many subnets are needed and how many hosts each subnet should support.\nCalculate Subnet Bits: Determine how many bits to borrow from the host portion of the address to create the required number of subnets.\nCalculate Host Bits: Ensure enough host bits remain to accommodate the required number of hosts per subnet.\nDetermine Subnet Mask: Calculate the new subnet mask based on the borrowed bits.\nCalculate Subnet Ranges: Identify the network address, usable IP range, and broadcast address for each subnet.\nBinary Calculation Method #\rFor those comfortable with binary notation, subnetting calculations can be simplified:\nConvert the original network address and subnet mask to binary. Determine the number of subnet bits needed. Create the subnets by incrementing the subnet bits. Convert the results back to decimal notation. Shortcut Methods #\rExperienced network administrators often use shortcut methods:\nPower of 2: Quickly calculate hosts per subnet using 2^n - 2, where n is the number of host bits. Subnet Increment: Determine the \u0026ldquo;jump size\u0026rdquo; between subnets based on the subnet mask. Real-World Applications #\rEnterprise Network Design #\rIn an enterprise environment, VLSM allows for efficient address allocation:\nLarge /23 subnets for departments with many devices Medium /24 subnets for average-sized departments Small /27 or /28 subnets for limited-device areas Tiny /30 subnets for router-to-router links ISP Address Allocation #\rInternet Service Providers use subnetting to allocate appropriate address blocks to customers of varying sizes, maximizing their available address space.\nData Center Segmentation #\rModern data centers leverage subnetting to create logical separations between different application tiers, management networks, and storage networks.\nLooking Forward: Subnetting in IPv6 #\rWhile this article focuses on IPv4 subnetting, it\u0026rsquo;s worth noting that IPv6 also employs subnetting principles, albeit with some differences:\nIPv6 uses a 128-bit address space (versus IPv4\u0026rsquo;s 32-bit) The standard subnet in IPv6 is a /64, leaving plenty of host addresses IPv6 simplifies some aspects of subnetting while introducing new considerations Conclusion #\rSubnetting stands as a cornerstone technique in network design and management, enabling organizations to create more efficient, manageable, and secure networks. The choice between FLSM and VLSM approaches depends on specific network requirements, with VLSM offering superior flexibility and efficiency for most modern networks.\nAs networks continue to grow in complexity and scale, mastering subnetting becomes increasingly valuable for IT professionals. Whether designing a new network from scratch or optimizing an existing infrastructure, the principles of effective subnetting remain essential to creating robust network architectures that can adapt to evolving business needs.\n","date":"15 April 2023","externalUrl":null,"permalink":"/posts/understanding-ip-subnetting/","section":"Posts","summary":"Master the art of IP subnetting with detailed explanations of subnet calculations, comparison of Fixed Length and Variable Length Subnet Masking, and real-world implementation strategies for network optimization.","title":"Understanding IP Subnetting: A Comprehensive Guide","type":"posts"},{"content":"","date":"15 April 2023","externalUrl":null,"permalink":"/tags/vlsm/","section":"Tags","summary":"","title":"VLSM","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]